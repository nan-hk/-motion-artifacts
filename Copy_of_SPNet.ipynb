{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nan-hk/-motion-artifacts/blob/master/Copy_of_SPNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "3846c946",
      "metadata": {
        "id": "3846c946",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab1b0274-2f4d-46b5-d2fc-3195ecff67e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.5-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (1.21.6)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (3.17.3)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.5\n"
          ]
        }
      ],
      "source": [
        "#train\n",
        "!pip install tensorboardX --no-cache-dir"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "zip_ref = zipfile.ZipFile(\"/content/drive/My Drive/TrainDataset1.zip\", 'r')\n",
        "zip_ref.extractall(\"/content/tmp\")\n",
        "zip_ref.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aBtOfacIz2qa",
        "outputId": "855d44f0-7e41-4c61-f55e-c4c4e626242e"
      },
      "id": "aBtOfacIz2qa",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "zip_ref = zipfile.ZipFile(\"/content/drive/My Drive/TestDataset1.zip\", 'r')\n",
        "zip_ref.extractall(\"/content/tmp\")\n",
        "zip_ref.close()"
      ],
      "metadata": {
        "id": "kgP-kzKD48IW"
      },
      "id": "kgP-kzKD48IW",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Rest2Net model"
      ],
      "metadata": {
        "id": "T7c0BF7hggkP"
      },
      "id": "T7c0BF7hggkP"
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "9NJ4zYYPgevn"
      },
      "id": "9NJ4zYYPgevn"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "068f0375",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "068f0375",
        "outputId": "28be80ab-6d77-44a2-f2bf-78a38ebc22e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 1000])\n"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "import math\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "__all__ = ['Res2Net', 'res2net50_v1b', 'res2net101_v1b']\n",
        "\n",
        "\n",
        "model_urls = {\n",
        "    'res2net50_v1b_26w_4s': 'https://shanghuagao.oss-cn-beijing.aliyuncs.com/res2net/res2net50_v1b_26w_4s-3cf99910.pth',\n",
        "    'res2net101_v1b_26w_4s': 'https://shanghuagao.oss-cn-beijing.aliyuncs.com/res2net/res2net101_v1b_26w_4s-0812c246.pth',\n",
        "}\n",
        "\n",
        "\n",
        "class Bottle2neck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, baseWidth=26, scale = 4, stype='normal'):\n",
        "        \"\"\" Constructor\n",
        "        Args:\n",
        "            inplanes: input channel dimensionality\n",
        "            planes: output channel dimensionality\n",
        "            stride: conv stride. Replaces pooling layer.\n",
        "            downsample: None when stride = 1\n",
        "            baseWidth: basic width of conv3x3\n",
        "            scale: number of scale.\n",
        "            type: 'normal': normal set. 'stage': first block of a new stage.\n",
        "        \"\"\"\n",
        "        super(Bottle2neck, self).__init__()\n",
        "\n",
        "        width = int(math.floor(planes * (baseWidth/64.0)))\n",
        "        self.conv1 = nn.Conv2d(inplanes, width*scale, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(width*scale)\n",
        "        \n",
        "        if scale == 1:\n",
        "          self.nums = 1\n",
        "        else:\n",
        "          self.nums = scale -1\n",
        "        if stype == 'stage':\n",
        "            self.pool = nn.AvgPool2d(kernel_size=3, stride = stride, padding=1)\n",
        "        convs = []\n",
        "        bns = []\n",
        "        for i in range(self.nums):\n",
        "          convs.append(nn.Conv2d(width, width, kernel_size=3, stride = stride, padding=1, bias=False))\n",
        "          bns.append(nn.BatchNorm2d(width))\n",
        "        self.convs = nn.ModuleList(convs)\n",
        "        self.bns = nn.ModuleList(bns)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(width*scale, planes * self.expansion, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n",
        "\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stype = stype\n",
        "        self.scale = scale\n",
        "        self.width  = width\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        spx = torch.split(out, self.width, 1)\n",
        "        for i in range(self.nums):\n",
        "          if i==0 or self.stype=='stage':\n",
        "            sp = spx[i]\n",
        "          else:\n",
        "            sp = sp + spx[i]\n",
        "          sp = self.convs[i](sp)\n",
        "          sp = self.relu(self.bns[i](sp))\n",
        "          if i==0:\n",
        "            out = sp\n",
        "          else:\n",
        "            out = torch.cat((out, sp), 1)\n",
        "        if self.scale != 1 and self.stype=='normal':\n",
        "          out = torch.cat((out, spx[self.nums]),1)\n",
        "        elif self.scale != 1 and self.stype=='stage':\n",
        "          out = torch.cat((out, self.pool(spx[self.nums])),1)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Res2Net(nn.Module):\n",
        "\n",
        "    def __init__(self, block, layers, baseWidth = 26, scale = 4, num_classes=1000):\n",
        "        self.inplanes = 64\n",
        "        super(Res2Net, self).__init__()\n",
        "        self.baseWidth = baseWidth\n",
        "        self.scale = scale\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 3, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(32, 32, 3, 1, 1, bias=False),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(32, 64, 3, 1, 1, bias=False)\n",
        "        )\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.AvgPool2d(kernel_size=stride, stride=stride, \n",
        "                    ceil_mode=True, count_include_pad=False),\n",
        "                nn.Conv2d(self.inplanes, planes * block.expansion, \n",
        "                    kernel_size=1, stride=1, bias=False),\n",
        "                nn.BatchNorm2d(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample=downsample, \n",
        "                        stype='stage', baseWidth = self.baseWidth, scale=self.scale))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes, baseWidth = self.baseWidth, scale=self.scale))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x0 = self.maxpool(x)\n",
        "        \n",
        "\n",
        "        x1 = self.layer1(x0)\n",
        "        x2 = self.layer2(x1)\n",
        "        x3 = self.layer3(x2)\n",
        "        x4 = self.layer4(x3)\n",
        "\n",
        "        x5 = self.avgpool(x4)\n",
        "        x6 = x5.view(x5.size(0), -1)\n",
        "        x7 = self.fc(x6)\n",
        "\n",
        "        return x7\n",
        "\n",
        "\n",
        "\n",
        "class Res2Net_Ours(nn.Module):\n",
        "\n",
        "    def __init__(self, block, layers, baseWidth = 26, scale = 4, num_classes=1000):\n",
        "        self.inplanes = 64\n",
        "        super(Res2Net_Ours, self).__init__()\n",
        "        \n",
        "        self.baseWidth = baseWidth\n",
        "        self.scale = scale\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 3, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(32, 32, 3, 1, 1, bias=False),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(32, 64, 3, 1, 1, bias=False)\n",
        "        )\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
        "       \n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.AvgPool2d(kernel_size=stride, stride=stride, \n",
        "                    ceil_mode=True, count_include_pad=False),\n",
        "                nn.Conv2d(self.inplanes, planes * block.expansion, \n",
        "                    kernel_size=1, stride=1, bias=False),\n",
        "                nn.BatchNorm2d(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample=downsample, \n",
        "                        stype='stage', baseWidth = self.baseWidth, scale=self.scale))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes, baseWidth = self.baseWidth, scale=self.scale))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x0 = self.maxpool(x)\n",
        "        \n",
        "\n",
        "        x1 = self.layer1(x0)\n",
        "        x2 = self.layer2(x1)\n",
        "        x3 = self.layer3(x2)\n",
        "        x4 = self.layer4(x3)\n",
        "\n",
        "\n",
        "        return x0,x1,x2,x3,x4\n",
        "    \n",
        "    \n",
        "\n",
        "def res2net50_v1b(pretrained=False, **kwargs):\n",
        "    \"\"\"Constructs a Res2Net-50_v1b model.\n",
        "    Res2Net-50 refers to the Res2Net-50_v1b_26w_4s.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = Res2Net(Bottle2neck, [3, 4, 6, 3], baseWidth = 26, scale = 4, **kwargs)\n",
        "    if pretrained:\n",
        "        model.load_state_dict(model_zoo.load_url(model_urls['res2net50_v1b_26w_4s'],map_location='cpu'))\n",
        "    return model\n",
        "\n",
        "def res2net101_v1b(pretrained=False, **kwargs):\n",
        "    \"\"\"Constructs a Res2Net-50_v1b_26w_4s model.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = Res2Net(Bottle2neck, [3, 4, 23, 3], baseWidth = 26, scale = 4, **kwargs)\n",
        "    if pretrained:\n",
        "        model.load_state_dict(model_zoo.load_url(model_urls['res2net101_v1b_26w_4s']))\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "def res2net50_v1b_Ours(pretrained=False, **kwargs):\n",
        "    \"\"\"Constructs a Res2Net-50_v1b model.\n",
        "    Res2Net-50 refers to the Res2Net-50_v1b_26w_4s.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = Res2Net_Ours(Bottle2neck, [3, 4, 6, 3], baseWidth = 26, scale = 4, **kwargs)\n",
        "    if pretrained:\n",
        "        model.load_state_dict(model_zoo.load_url(model_urls['res2net50_v1b_26w_4s']))\n",
        "    return model\n",
        "\n",
        "def res2net101_v1b_Ours(pretrained=False, **kwargs):\n",
        "    \"\"\"Constructs a Res2Net-50_v1b_26w_4s model.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = Res2Net_Ours(Bottle2neck, [3, 4, 23, 3], baseWidth = 26, scale = 4, **kwargs)\n",
        "    if pretrained:\n",
        "        model.load_state_dict(model_zoo.load_url(model_urls['res2net101_v1b_26w_4s']))\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "def res2net50_v1b_26w_4s(pretrained=False, **kwargs):\n",
        "    \"\"\"Constructs a Res2Net-50_v1b_26w_4s model.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = Res2Net(Bottle2neck, [3, 4, 6, 3], baseWidth = 26, scale = 4, **kwargs)\n",
        "    if pretrained:\n",
        "        model.load_state_dict(model_zoo.load_url(model_urls['res2net50_v1b_26w_4s'],map_location='cpu'))\n",
        "    return model\n",
        "\n",
        "def res2net101_v1b_26w_4s(pretrained=False, **kwargs):\n",
        "    \"\"\"Constructs a Res2Net-50_v1b_26w_4s model.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = Res2Net(Bottle2neck, [3, 4, 23, 3], baseWidth = 26, scale = 4, **kwargs)\n",
        "    if pretrained:\n",
        "        model.load_state_dict(model_zoo.load_url(model_urls['res2net101_v1b_26w_4s']))\n",
        "    return model\n",
        "\n",
        "def res2net152_v1b_26w_4s(pretrained=False, **kwargs):\n",
        "    \"\"\"Constructs a Res2Net-50_v1b_26w_4s model.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = Res2Net(Bottle2neck, [3, 8, 36, 3], baseWidth = 26, scale = 4, **kwargs)\n",
        "    if pretrained:\n",
        "        model.load_state_dict(model_zoo.load_url(model_urls['res2net152_v1b_26w_4s']))\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "   \n",
        "def Res2Net_model(ind=50):\n",
        "    \n",
        "    if ind == 50:\n",
        "        model_base = res2net50_v1b(pretrained=True)\n",
        "        model      = res2net50_v1b_Ours()\n",
        "\n",
        "    if ind == 101:\n",
        "        model_base = res2net101_v1b(pretrained=True)\n",
        "        model      = res2net101_v1b_Ours()\n",
        "        \n",
        "        \n",
        "    pretrained_dict = model_base.state_dict()\n",
        "    model_dict      = model.state_dict()\n",
        "    \n",
        "    pretrained_dict =  {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
        "    \n",
        "    model_dict.update(pretrained_dict)\n",
        "    model.load_state_dict(model_dict)\n",
        "    \n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    images = torch.rand(1, 3, 352, 352)\n",
        "    model = res2net50_v1b_26w_4s(pretrained=False)\n",
        "    model = model\n",
        "    print(model(images).size())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85810b84",
      "metadata": {
        "id": "85810b84"
      },
      "outputs": [],
      "source": [
        "#SPNet Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "bbe5d415",
      "metadata": {
        "id": "bbe5d415"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "from torch.nn import functional as F\n",
        "\n",
        "def maxpool():\n",
        "    pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "    return pool\n",
        "\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    \"3x3 convolution with padding\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=1, bias=False)\n",
        "\n",
        "\n",
        "class BasicConv2d(nn.Module):\n",
        "    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=0, dilation=1):\n",
        "        super(BasicConv2d, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_planes, out_planes,\n",
        "                              kernel_size=kernel_size, stride=stride,\n",
        "                              padding=padding, dilation=dilation, bias=False)\n",
        "        self.bn = nn.BatchNorm2d(out_planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.bn(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "#Global Contextual module\n",
        "class GCM(nn.Module):\n",
        "    def __init__(self, in_channel, out_channel):\n",
        "        super(GCM, self).__init__()\n",
        "        self.relu = nn.ReLU(True)\n",
        "        self.branch0 = nn.Sequential(\n",
        "            BasicConv2d(in_channel, out_channel, 1),\n",
        "        )\n",
        "        self.branch1 = nn.Sequential(\n",
        "            BasicConv2d(in_channel, out_channel, 1),\n",
        "            BasicConv2d(out_channel, out_channel, kernel_size=(1, 3), padding=(0, 1)),\n",
        "            BasicConv2d(out_channel, out_channel, kernel_size=(3, 1), padding=(1, 0)),\n",
        "            BasicConv2d(out_channel, out_channel, 3, padding=3, dilation=3)\n",
        "        )\n",
        "        self.branch2 = nn.Sequential(\n",
        "            BasicConv2d(in_channel, out_channel, 1),\n",
        "            BasicConv2d(out_channel, out_channel, kernel_size=(1, 5), padding=(0, 2)),\n",
        "            BasicConv2d(out_channel, out_channel, kernel_size=(5, 1), padding=(2, 0)),\n",
        "            BasicConv2d(out_channel, out_channel, 3, padding=5, dilation=5)\n",
        "        )\n",
        "        self.branch3 = nn.Sequential(\n",
        "            BasicConv2d(in_channel, out_channel, 1),\n",
        "            BasicConv2d(out_channel, out_channel, kernel_size=(1, 7), padding=(0, 3)),\n",
        "            BasicConv2d(out_channel, out_channel, kernel_size=(7, 1), padding=(3, 0)),\n",
        "            BasicConv2d(out_channel, out_channel, 3, padding=7, dilation=7)\n",
        "        )\n",
        "        self.conv_cat = BasicConv2d(4*out_channel, out_channel, 3, padding=1)\n",
        "        self.conv_res = BasicConv2d(in_channel, out_channel, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x0 = self.branch0(x)\n",
        "        x1 = self.branch1(x)\n",
        "        x2 = self.branch2(x)\n",
        "        x3 = self.branch3(x)\n",
        "\n",
        "        x_cat = self.conv_cat(torch.cat((x0, x1, x2, x3), 1))\n",
        "\n",
        "        x = self.relu(x_cat + self.conv_res(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "\n",
        "class CIM0(nn.Module):    \n",
        "    def __init__(self,in_dim, out_dim):\n",
        "        super(CIM0, self).__init__()\n",
        "        \n",
        "        act_fn = nn.ReLU(inplace=True)\n",
        "        \n",
        "\n",
        "        self.layer_10 = nn.Conv2d(out_dim, out_dim, kernel_size=3, stride=1, padding=1)\n",
        "        self.layer_20 = nn.Conv2d(out_dim, out_dim, kernel_size=3, stride=1, padding=1)   \n",
        "        \n",
        "        self.layer_11 = nn.Sequential(nn.Conv2d(out_dim, out_dim, kernel_size=3, stride=1, padding=1),nn.BatchNorm2d(out_dim),act_fn,)        \n",
        "        self.layer_21 = nn.Sequential(nn.Conv2d(out_dim, out_dim, kernel_size=3, stride=1, padding=1),nn.BatchNorm2d(out_dim),act_fn,)\n",
        "        \n",
        "        self.gamma1 = nn.Parameter(torch.zeros(1))\n",
        "        self.gamma2 = nn.Parameter(torch.zeros(1))\n",
        "        \n",
        "\n",
        "        self.layer_ful1 = nn.Sequential(nn.Conv2d(out_dim*2, out_dim, kernel_size=3, stride=1, padding=1),nn.BatchNorm2d(out_dim),act_fn,)\n",
        "        \n",
        "\n",
        "    def forward(self, rgb, depth):\n",
        "        \n",
        "        ################################\n",
        "        \n",
        "        x_rgb = self.layer_10(rgb)\n",
        "        x_dep = self.layer_20(depth)\n",
        "        \n",
        "        rgb_w = nn.Sigmoid()(x_rgb)\n",
        "        dep_w = nn.Sigmoid()(x_dep)\n",
        "        \n",
        "        ##\n",
        "        x_rgb_w = rgb.mul(dep_w)\n",
        "        x_dep_w = depth.mul(rgb_w)\n",
        "        \n",
        "        x_rgb_r = x_rgb_w + rgb\n",
        "        x_dep_r = x_dep_w + depth\n",
        "        \n",
        "        ## fusion \n",
        "        x_rgb_r = self.layer_11(x_rgb_r)\n",
        "        x_dep_r = self.layer_21(x_dep_r)\n",
        "        \n",
        "        \n",
        "        ful_mul = torch.mul(x_rgb_r, x_dep_r)         \n",
        "        x_in1   = torch.reshape(x_rgb_r,[x_rgb_r.shape[0],1,x_rgb_r.shape[1],x_rgb_r.shape[2],x_rgb_r.shape[3]])\n",
        "        x_in2   = torch.reshape(x_dep_r,[x_dep_r.shape[0],1,x_dep_r.shape[1],x_dep_r.shape[2],x_dep_r.shape[3]])\n",
        "        x_cat   = torch.cat((x_in1, x_in2),dim=1)\n",
        "        ful_max = x_cat.max(dim=1)[0]\n",
        "        ful_out = torch.cat((ful_mul,ful_max),dim=1)\n",
        "        \n",
        "        out1 = self.layer_ful1(ful_out)\n",
        "         \n",
        "        return out1\n",
        "\n",
        "\n",
        "class CIM(nn.Module):    \n",
        "    def __init__(self,in_dim, out_dim):\n",
        "        super(CIM, self).__init__()\n",
        "        \n",
        "        act_fn = nn.ReLU(inplace=True)\n",
        "        \n",
        "        self.reduc_1 = nn.Sequential(nn.Conv2d(in_dim, out_dim, kernel_size=1), act_fn)\n",
        "        self.reduc_2 = nn.Sequential(nn.Conv2d(in_dim, out_dim, kernel_size=1), act_fn)\n",
        "        \n",
        "        self.layer_10 = nn.Conv2d(out_dim, out_dim, kernel_size=3, stride=1, padding=1)\n",
        "        self.layer_20 = nn.Conv2d(out_dim, out_dim, kernel_size=3, stride=1, padding=1)   \n",
        "        \n",
        "        self.layer_11 = nn.Sequential(nn.Conv2d(out_dim, out_dim, kernel_size=3, stride=1, padding=1),nn.BatchNorm2d(out_dim),act_fn,)        \n",
        "        self.layer_21 = nn.Sequential(nn.Conv2d(out_dim, out_dim, kernel_size=3, stride=1, padding=1),nn.BatchNorm2d(out_dim),act_fn,)\n",
        "        \n",
        "        self.gamma1 = nn.Parameter(torch.zeros(1))\n",
        "        self.gamma2 = nn.Parameter(torch.zeros(1))\n",
        "        \n",
        "\n",
        "        self.layer_ful1 = nn.Sequential(nn.Conv2d(out_dim*2, out_dim, kernel_size=3, stride=1, padding=1),nn.BatchNorm2d(out_dim),act_fn,)\n",
        "        self.layer_ful2 = nn.Sequential(nn.Conv2d(out_dim+out_dim//2, out_dim, kernel_size=3, stride=1, padding=1),nn.BatchNorm2d(out_dim),act_fn,)\n",
        "\n",
        "    def forward(self, rgb, depth, xx):\n",
        "        \n",
        "        ################################\n",
        "        x_rgb = self.reduc_1(rgb)\n",
        "        x_dep = self.reduc_2(depth)\n",
        "        \n",
        "        x_rgb1 = self.layer_10(x_rgb)\n",
        "        x_dep1 = self.layer_20(x_dep)\n",
        "        \n",
        "        rgb_w = nn.Sigmoid()(x_rgb1)\n",
        "        dep_w = nn.Sigmoid()(x_dep1)\n",
        "        \n",
        "        ##\n",
        "        x_rgb_w = x_rgb.mul(dep_w)\n",
        "        x_dep_w = x_dep.mul(rgb_w)\n",
        "        \n",
        "        x_rgb_r = x_rgb_w + x_rgb\n",
        "        x_dep_r = x_dep_w + x_dep\n",
        "        \n",
        "        ## fusion \n",
        "        x_rgb_r = self.layer_11(x_rgb_r)\n",
        "        x_dep_r = self.layer_21(x_dep_r)\n",
        "        \n",
        "        \n",
        "        ful_mul = torch.mul(x_rgb_r, x_dep_r)         \n",
        "        x_in1   = torch.reshape(x_rgb_r,[x_rgb_r.shape[0],1,x_rgb_r.shape[1],x_rgb_r.shape[2],x_rgb_r.shape[3]])\n",
        "        x_in2   = torch.reshape(x_dep_r,[x_dep_r.shape[0],1,x_dep_r.shape[1],x_dep_r.shape[2],x_dep_r.shape[3]])\n",
        "        x_cat   = torch.cat((x_in1, x_in2),dim=1)\n",
        "        ful_max = x_cat.max(dim=1)[0]\n",
        "        ful_out = torch.cat((ful_mul,ful_max),dim=1)\n",
        "        \n",
        "        out1 = self.layer_ful1(ful_out)\n",
        "        out2 = self.layer_ful2(torch.cat([out1,xx],dim=1))\n",
        "         \n",
        "        return out2\n",
        "\n",
        "\n",
        "\n",
        "class MFA(nn.Module):    \n",
        "    def __init__(self,in_dim):\n",
        "        super(MFA, self).__init__()\n",
        "         \n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        \n",
        "        self.layer_10 = nn.Conv2d(in_dim, in_dim, kernel_size=3, stride=1, padding=1)\n",
        "        self.layer_20 = nn.Conv2d(in_dim, in_dim, kernel_size=3, stride=1, padding=1)   \n",
        "        self.layer_cat1 = nn.Sequential(nn.Conv2d(in_dim*2, in_dim, kernel_size=3, stride=1, padding=1),nn.BatchNorm2d(in_dim),)        \n",
        "        \n",
        "    def forward(self, x_ful, x1, x2):\n",
        "        \n",
        "        ################################\n",
        "    \n",
        "        x_ful_1 = x_ful.mul(x1)\n",
        "        x_ful_2 = x_ful.mul(x2)\n",
        "        \n",
        "     \n",
        "        x_ful_w = self.layer_cat1(torch.cat([x_ful_1, x_ful_2],dim=1))\n",
        "        out     = self.relu(x_ful + x_ful_w)\n",
        "        \n",
        "        return out\n",
        "    \n",
        "    \n",
        "\n",
        "  \n",
        "   \n",
        "###############################################################################\n",
        "\n",
        "class SPNet(nn.Module):\n",
        "    def __init__(self, channel=32,ind=50):\n",
        "        super(SPNet, self).__init__()\n",
        "        \n",
        "       \n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        \n",
        "        self.upsample_2 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "        self.upsample_4 = nn.Upsample(scale_factor=4, mode='bilinear', align_corners=True)\n",
        "        self.upsample_8 = nn.Upsample(scale_factor=8, mode='bilinear', align_corners=True)\n",
        "        \n",
        "        #Backbone model\n",
        "        #Backbone model\n",
        "        self.layer_rgb  = Res2Net_model(ind)\n",
        "        self.layer_dep  = Res2Net_model(ind)\n",
        "        \n",
        "        self.layer_dep0 = nn.Conv2d(1, 3, kernel_size=1)\n",
        "        \n",
        "        ###############################################\n",
        "        # funsion encoders #\n",
        "        ###############################################\n",
        "        self.fu_0 = CIM0(64, 64)#\n",
        "        \n",
        "        self.fu_1 = CIM(256, 128) #MixedFusion_Block_IMfusion\n",
        "        self.pool_fu_1 = maxpool()\n",
        "        \n",
        "        self.fu_2 = CIM(512, 256)\n",
        "        self.pool_fu_2 = maxpool()\n",
        "        \n",
        "        self.fu_3 = CIM(1024, 512)\n",
        "        self.pool_fu_3 = maxpool()\n",
        "\n",
        "        self.fu_4 = CIM(2048, 1024)\n",
        "        self.pool_fu_4 = maxpool()\n",
        "        \n",
        "        \n",
        "        ###############################################\n",
        "        # decoders #\n",
        "        ###############################################\n",
        "        \n",
        "        ## rgb\n",
        "        self.rgb_conv_4   = nn.Sequential(BasicConv2d(2048,    256, 3, padding=1),self.relu)\n",
        "        self.rgb_gcm_4    = GCM(2048,  channel)\n",
        "        \n",
        "        self.rgb_conv_3   = nn.Sequential(BasicConv2d(1024+32, 256, 3, padding=1),self.relu)\n",
        "        self.rgb_gcm_3    = GCM(1024+32,  channel)\n",
        "\n",
        "        self.rgb_conv_2   = nn.Sequential(BasicConv2d(512+32, 128, 3, padding=1),self.relu)\n",
        "        self.rgb_gcm_2    = GCM(512+32,  channel)\n",
        "\n",
        "        self.rgb_conv_1   = nn.Sequential(BasicConv2d(256+32, 128, 3, padding=1),self.relu)\n",
        "        self.rgb_gcm_1    = GCM(256+32,  channel)\n",
        "\n",
        "        self.rgb_conv_0   = nn.Sequential(BasicConv2d(64+32, 64, 3, padding=1),self.relu)\n",
        "        self.rgb_gcm_0    = GCM(64+32,  channel)        \n",
        "        self.rgb_conv_out = nn.Conv2d(channel, 1, 1)\n",
        "        \n",
        "        ## depth\n",
        "        self.dep_conv_4   = nn.Sequential(BasicConv2d(2048, 256, 3, padding=1),self.relu)\n",
        "        self.dep_gcm_4    = GCM(2048,  channel)\n",
        "        \n",
        "        self.dep_conv_3   = nn.Sequential(BasicConv2d(1024+32, 256, 3, padding=1),self.relu)\n",
        "        self.dep_gcm_3    = GCM(1024+32,  channel)\n",
        "\n",
        "        self.dep_conv_2   = nn.Sequential(BasicConv2d(512+32, 128, 3, padding=1),self.relu)\n",
        "        self.dep_gcm_2    = GCM(512+32,  channel)\n",
        "\n",
        "        self.dep_conv_1   = nn.Sequential(BasicConv2d(256+32, 128, 3, padding=1),self.relu)\n",
        "        self.dep_gcm_1    = GCM(256+32,  channel)\n",
        "\n",
        "        self.dep_conv_0   = nn.Sequential(BasicConv2d(64+32, 64, 3, padding=1),self.relu)\n",
        "        self.dep_gcm_0    = GCM(64+32,  channel)        \n",
        "        self.dep_conv_out = nn.Conv2d(channel, 1, 1)\n",
        "        \n",
        "        ## fusion\n",
        "        self.ful_conv_4   = nn.Sequential(BasicConv2d(2048, 256, 3, padding=1),self.relu)\n",
        "        self.ful_gcm_4    = GCM(1024,  channel)\n",
        "        \n",
        "        self.ful_conv_3   = nn.Sequential(BasicConv2d(1024+32*3, 256, 3, padding=1),self.relu)\n",
        "        self.ful_gcm_3    = GCM(512+32,  channel)\n",
        "\n",
        "        self.ful_conv_2   = nn.Sequential(BasicConv2d(512+32*3, 128, 3, padding=1),self.relu)\n",
        "        self.ful_gcm_2    = GCM(256+32,  channel)\n",
        "\n",
        "        self.ful_conv_1   = nn.Sequential(BasicConv2d(256+32*3, 128, 3, padding=1),self.relu)\n",
        "        self.ful_gcm_1    = GCM(128+32,  channel)\n",
        "\n",
        "        self.ful_conv_0   = nn.Sequential(BasicConv2d(128+32*3, 64, 3, padding=1),self.relu)\n",
        "        self.ful_gcm_0    = GCM(64+32,  channel)        \n",
        "        self.ful_conv_out = nn.Conv2d(channel, 1, 1)\n",
        "        \n",
        "        self.ful_layer4   = MFA(channel)\n",
        "        self.ful_layer3   = MFA(channel)\n",
        "        self.ful_layer2   = MFA(channel)\n",
        "        self.ful_layer1   = MFA(channel)\n",
        "        self.ful_layer0   = MFA(channel)\n",
        "        \n",
        "                \n",
        "\n",
        "    def forward(self, imgs, depths):\n",
        "        \n",
        "        img_0, img_1, img_2, img_3, img_4 = self.layer_rgb(imgs)\n",
        "        dep_0, dep_1, dep_2, dep_3, dep_4 = self.layer_dep(self.layer_dep0(depths))\n",
        "        \n",
        "    \n",
        "      \n",
        "        ####################################################\n",
        "        ## fusion\n",
        "        ####################################################\n",
        "        ful_0    = self.fu_0(img_0, dep_0)\n",
        "        ful_1    = self.fu_1(img_1, dep_1, ful_0)\n",
        "        ful_2    = self.fu_2(img_2, dep_2, self.pool_fu_1(ful_1))\n",
        "        ful_3    = self.fu_3(img_3, dep_3, self.pool_fu_2(ful_2))\n",
        "        ful_4    = self.fu_4(img_4, dep_4, self.pool_fu_3(ful_3))\n",
        "        \n",
        "        ####################################################\n",
        "        ## decoder rgb\n",
        "        ####################################################        \n",
        "        #\n",
        "        x_rgb_42    = self.rgb_gcm_4(img_4)\n",
        "        \n",
        "        x_rgb_3_cat = torch.cat([img_3, self.upsample_2(x_rgb_42)], dim=1)\n",
        "        x_rgb_32    = self.rgb_gcm_3(x_rgb_3_cat)\n",
        "        \n",
        "        x_rgb_2_cat = torch.cat([img_2, self.upsample_2(x_rgb_32)], dim=1)\n",
        "        x_rgb_22    = self.rgb_gcm_2(x_rgb_2_cat)        \n",
        "\n",
        "        x_rgb_1_cat = torch.cat([img_1, self.upsample_2(x_rgb_22)], dim=1)\n",
        "        x_rgb_12    = self.rgb_gcm_1(x_rgb_1_cat)     \n",
        "\n",
        "        x_rgb_0_cat = torch.cat([img_0, x_rgb_12], dim=1)\n",
        "        x_rgb_02    = self.rgb_gcm_0(x_rgb_0_cat)     \n",
        "        rgb_out     = self.upsample_4(self.rgb_conv_out(x_rgb_02))\n",
        "        \n",
        "        \n",
        "        ####################################################\n",
        "        ## decoder depth\n",
        "        ####################################################        \n",
        "        #\n",
        "        x_dep_42    = self.dep_gcm_4(dep_4)\n",
        "        \n",
        "        x_dep_3_cat = torch.cat([dep_3, self.upsample_2(x_dep_42)], dim=1)\n",
        "        x_dep_32    = self.dep_gcm_3(x_dep_3_cat)\n",
        "        \n",
        "        x_dep_2_cat = torch.cat([dep_2, self.upsample_2(x_dep_32)], dim=1)\n",
        "        x_dep_22    = self.dep_gcm_2(x_dep_2_cat)        \n",
        "\n",
        "        x_dep_1_cat = torch.cat([dep_1, self.upsample_2(x_dep_22)], dim=1)\n",
        "        x_dep_12    = self.dep_gcm_1(x_dep_1_cat)     \n",
        "\n",
        "        x_dep_0_cat = torch.cat([dep_0, x_dep_12], dim=1)\n",
        "        x_dep_02    = self.dep_gcm_0(x_dep_0_cat)     \n",
        "        dep_out     = self.upsample_4(self.dep_conv_out(x_dep_02))\n",
        "        \n",
        "\n",
        "        ####################################################\n",
        "        ## decoder fusion\n",
        "        ####################################################        \n",
        "        #\n",
        "        x_ful_42    = self.ful_gcm_4(ful_4)\n",
        "        \n",
        "        x_ful_3_cat = torch.cat([ful_3, self.ful_layer3(self.upsample_2(x_ful_42),self.upsample_2(x_rgb_42),self.upsample_2(x_dep_42))], dim=1)\n",
        "        x_ful_32    = self.ful_gcm_3(x_ful_3_cat)\n",
        "        \n",
        "        x_ful_2_cat = torch.cat([ful_2, self.ful_layer2(self.upsample_2(x_ful_32),self.upsample_2(x_rgb_32),self.upsample_2(x_dep_32))], dim=1)\n",
        "        x_ful_22    = self.ful_gcm_2(x_ful_2_cat)        \n",
        "\n",
        "        x_ful_1_cat = torch.cat([ful_1, self.ful_layer1(self.upsample_2(x_ful_22),self.upsample_2(x_rgb_22),self.upsample_2(x_dep_22))], dim=1)\n",
        "        x_ful_12    = self.ful_gcm_1(x_ful_1_cat)     \n",
        "\n",
        "        x_ful_0_cat = torch.cat([ful_0, self.ful_layer0(x_ful_12, x_rgb_12, x_dep_12)], dim=1)\n",
        "        x_ful_02    = self.ful_gcm_0(x_ful_0_cat)     \n",
        "        ful_out     = self.upsample_4(self.ful_conv_out(x_ful_02))\n",
        "\n",
        "\n",
        "        return rgb_out, dep_out, ful_out\n",
        "    \n",
        "    \n",
        "\n",
        "    def _make_agant_layer(self, inplanes, planes):\n",
        "        layers = nn.Sequential(\n",
        "            nn.Conv2d(inplanes, planes, kernel_size=1,\n",
        "                      stride=1, padding=0, bias=False),\n",
        "            nn.BatchNorm2d(planes),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        return layers\n",
        "\n",
        "    def _make_transpose(self, block, planes, blocks, stride=1):\n",
        "        upsample = None\n",
        "        if stride != 1:\n",
        "            upsample = nn.Sequential(\n",
        "                nn.ConvTranspose2d(self.inplanes, planes,\n",
        "                                   kernel_size=2, stride=stride,\n",
        "                                   padding=0, bias=False),\n",
        "                nn.BatchNorm2d(planes),\n",
        "            )\n",
        "        elif self.inplanes != planes:\n",
        "            upsample = nn.Sequential(\n",
        "                nn.Conv2d(self.inplanes, planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, self.inplanes))\n",
        "\n",
        "        layers.append(block(self.inplanes, planes, stride, upsample))\n",
        "        self.inplanes = planes\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "    \n",
        "   \n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d39ea02d",
      "metadata": {
        "id": "d39ea02d"
      },
      "outputs": [],
      "source": [
        "#ResNet model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "1dfdaf50",
      "metadata": {
        "id": "1dfdaf50"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=1, bias=False)\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
        "                               padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet50(nn.Module):\n",
        "    def __init__(self,mode='rgb'):\n",
        "        self.inplanes = 64\n",
        "        super(ResNet50, self).__init__()\n",
        "        if(mode=='rgb'):\n",
        "            self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n",
        "                               bias=False)\n",
        "        elif(mode=='rgbd'):\n",
        "            self.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3,\n",
        "                               bias=False)\n",
        "        elif(mode==\"share\"):\n",
        "            self.conv1=nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n",
        "                               bias=False)\n",
        "            self.conv1_d=nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3,\n",
        "                               bias=False)\n",
        "        else:\n",
        "            raise \n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(Bottleneck, 64, 3)\n",
        "        self.layer2 = self._make_layer(Bottleneck, 128, 4, stride=2)\n",
        "        self.layer3_1 = self._make_layer(Bottleneck, 256, 6, stride=2)\n",
        "        self.layer4_1 = self._make_layer(Bottleneck, 512, 3, stride=2)\n",
        "\n",
        "        self.inplanes = 512\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x1 = self.layer3_1(x)\n",
        "        x1 = self.layer4_1(x1)\n",
        "\n",
        "        return x1, x1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c67c8a5",
      "metadata": {
        "id": "3c67c8a5"
      },
      "outputs": [],
      "source": [
        "#Data "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "e8d4d20a",
      "metadata": {
        "id": "e8d4d20a"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import torch.utils.data as data\n",
        "import torchvision.transforms as transforms\n",
        "import random\n",
        "import numpy as np\n",
        "from PIL import ImageEnhance\n",
        "\n",
        "#several data augumentation strategies\n",
        "def cv_random_flip(img, label,depth):\n",
        "    flip_flag = random.randint(0, 1)\n",
        "    # flip_flag2= random.randint(0,1)\n",
        "    #left right flip\n",
        "    if flip_flag == 1:\n",
        "        img = img.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "        label = label.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "        depth = depth.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "    #top bottom flip\n",
        "    # if flip_flag2==1:\n",
        "    #     img = img.transpose(Image.FLIP_TOP_BOTTOM)\n",
        "    #     label = label.transpose(Image.FLIP_TOP_BOTTOM)\n",
        "    #     depth = depth.transpose(Image.FLIP_TOP_BOTTOM)\n",
        "    return img, label, depth\n",
        "def randomCrop(image, label,depth):\n",
        "    border=30\n",
        "    image_width = image.size[0]\n",
        "    image_height = image.size[1]\n",
        "    crop_win_width = np.random.randint(image_width-border , image_width)\n",
        "    crop_win_height = np.random.randint(image_height-border , image_height)\n",
        "    random_region = (\n",
        "        (image_width - crop_win_width) >> 1, (image_height - crop_win_height) >> 1, (image_width + crop_win_width) >> 1,\n",
        "        (image_height + crop_win_height) >> 1)\n",
        "    return image.crop(random_region), label.crop(random_region),depth.crop(random_region)\n",
        "def randomRotation(image,label,depth):\n",
        "    mode=Image.BICUBIC\n",
        "    if random.random()>0.8:\n",
        "        random_angle = np.random.randint(-15, 15)\n",
        "        image=image.rotate(random_angle, mode)\n",
        "        label=label.rotate(random_angle, mode)\n",
        "        depth=depth.rotate(random_angle, mode)\n",
        "    return image,label,depth\n",
        "def colorEnhance(image):\n",
        "    bright_intensity=random.randint(5,15)/10.0\n",
        "    image=ImageEnhance.Brightness(image).enhance(bright_intensity)\n",
        "    contrast_intensity=random.randint(5,15)/10.0\n",
        "    image=ImageEnhance.Contrast(image).enhance(contrast_intensity)\n",
        "    color_intensity=random.randint(0,20)/10.0\n",
        "    image=ImageEnhance.Color(image).enhance(color_intensity)\n",
        "    sharp_intensity=random.randint(0,30)/10.0\n",
        "    image=ImageEnhance.Sharpness(image).enhance(sharp_intensity)\n",
        "    return image\n",
        "def randomGaussian(image, mean=0.1, sigma=0.35):\n",
        "    def gaussianNoisy(im, mean=mean, sigma=sigma):\n",
        "        for _i in range(len(im)):\n",
        "            im[_i] += random.gauss(mean, sigma)\n",
        "        return im\n",
        "    img = np.asarray(image)\n",
        "    width, height = img.shape\n",
        "    img = gaussianNoisy(img[:].flatten(), mean, sigma)\n",
        "    img = img.reshape([width, height])\n",
        "    return Image.fromarray(np.uint8(img))\n",
        "def randomPeper(img):\n",
        "\n",
        "    img=np.array(img)\n",
        "    noiseNum=int(0.0015*img.shape[0]*img.shape[1])\n",
        "    for i in range(noiseNum):\n",
        "\n",
        "        randX=random.randint(0,img.shape[0]-1)  \n",
        "\n",
        "        randY=random.randint(0,img.shape[1]-1)  \n",
        "\n",
        "        if random.randint(0,1)==0:  \n",
        "\n",
        "            img[randX,randY]=0  \n",
        "\n",
        "        else:  \n",
        "\n",
        "            img[randX,randY]=255 \n",
        "    return Image.fromarray(img)  \n",
        "\n",
        "# dataset for training\n",
        "#The current loader is not using the normalized depth maps for training and test. If you use the normalized depth maps\n",
        "#(e.g., 0 represents background and 1 represents foreground.), the performance will be further improved.\n",
        "\n",
        "class SalObjDataset(data.Dataset):\n",
        "    def __init__(self, image_root, gt_root,depth_root, trainsize):\n",
        "        self.trainsize = trainsize\n",
        "        self.images = [image_root + f for f in os.listdir(image_root) if f.endswith('.jpg') or f.endswith('.png')]\n",
        "        self.gts = [gt_root + f for f in os.listdir(gt_root) if f.endswith('.jpg')\n",
        "                    or f.endswith('.png')]\n",
        "        self.depths=[depth_root + f for f in os.listdir(depth_root) if f.endswith('.bmp')\n",
        "                    or f.endswith('.png')]\n",
        "        self.images = sorted(self.images)\n",
        "        self.gts = sorted(self.gts)\n",
        "        self.depths=sorted(self.depths)\n",
        "        print('SalObjDat', )\n",
        "        self.filter_files()\n",
        "        self.size = len(self.images)\n",
        "        self.img_transform = transforms.Compose([\n",
        "            transforms.Resize((self.trainsize, self.trainsize)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "        self.gt_transform = transforms.Compose([\n",
        "            transforms.Resize((self.trainsize, self.trainsize)),\n",
        "            transforms.ToTensor()])\n",
        "        self.depths_transform = transforms.Compose([transforms.Resize((self.trainsize, self.trainsize)),transforms.ToTensor()])\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image = self.rgb_loader(self.images[index])\n",
        "        gt = self.binary_loader(self.gts[index])\n",
        "        depth=self.binary_loader(self.depths[index])\n",
        "        image,gt,depth =cv_random_flip(image,gt,depth)\n",
        "        image,gt,depth=randomCrop(image, gt,depth)\n",
        "        image,gt,depth=randomRotation(image, gt,depth)\n",
        "        image=colorEnhance(image)\n",
        "        # gt=randomGaussian(gt)\n",
        "        gt=randomPeper(gt)\n",
        "        image = self.img_transform(image)\n",
        "        gt = self.gt_transform(gt)\n",
        "        depth=self.depths_transform(depth)\n",
        "        \n",
        "        return image, gt, depth\n",
        "\n",
        "    def filter_files(self):\n",
        "        print('SalObjDataset', self.images, self.gts)\n",
        "        assert len(self.images) == len(self.gts) and len(self.gts)==len(self.images)\n",
        "        images = []\n",
        "        gts = []\n",
        "        depths=[]\n",
        "        for img_path, gt_path,depth_path in zip(self.images, self.gts, self.depths):\n",
        "            img = Image.open(img_path)\n",
        "            gt = Image.open(gt_path)\n",
        "            depth= Image.open(depth_path)\n",
        "            if img.size == gt.size and gt.size==depth.size:\n",
        "                images.append(img_path)\n",
        "                gts.append(gt_path)\n",
        "                depths.append(depth_path)\n",
        "        self.images = images\n",
        "        self.gts = gts\n",
        "        self.depths=depths\n",
        "\n",
        "    def rgb_loader(self, path):\n",
        "        with open(path, 'rb') as f:\n",
        "            img = Image.open(f)\n",
        "            return img.convert('RGB')\n",
        "\n",
        "    def binary_loader(self, path):\n",
        "        with open(path, 'rb') as f:\n",
        "            img = Image.open(f)\n",
        "            return img.convert('L')\n",
        "\n",
        "    def resize(self, img, gt, depth):\n",
        "        assert img.size == gt.size and gt.size==depth.size\n",
        "        w, h = img.size\n",
        "        if h < self.trainsize or w < self.trainsize:\n",
        "            h = max(h, self.trainsize)\n",
        "            w = max(w, self.trainsize)\n",
        "            return img.resize((w, h), Image.BILINEAR), gt.resize((w, h), Image.NEAREST),depth.resize((w, h), Image.NEAREST)\n",
        "        else:\n",
        "            return img, gt, depth\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.size\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "# 0919\n",
        "#\n",
        "\n",
        "class SalObjDataset_var(data.Dataset):\n",
        "    def __init__(self, image_root, gt_root,depth_root, trainsize):\n",
        "        \n",
        "        self.trainsize = trainsize\n",
        "        self.images = [image_root + f for f in os.listdir(image_root) if f.endswith('.jpg')]\n",
        "        self.gts    = [gt_root + f for f in os.listdir(gt_root) if f.endswith('.jpg') or f.endswith('.png')]\n",
        "        self.depths = [depth_root + f for f in os.listdir(depth_root) if f.endswith('.bmp') or f.endswith('.png')]\n",
        "        self.images = sorted(self.images)\n",
        "        self.gts    = sorted(self.gts)\n",
        "        self.depths = sorted(self.depths)\n",
        "        self.filter_files()\n",
        "        self.size   = len(self.images)\n",
        "        \n",
        "        self.img_transform = transforms.Compose([\n",
        "            transforms.Resize((self.trainsize, self.trainsize)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "        self.gt_transform = transforms.Compose([\n",
        "            transforms.Resize((self.trainsize, self.trainsize)),\n",
        "            transforms.ToTensor()])\n",
        "        self.depths_transform = transforms.Compose([transforms.Resize((self.trainsize, self.trainsize)),transforms.ToTensor()])\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \n",
        "        ## read imag, gt, depth\n",
        "        image0 = self.rgb_loader(self.images[index])\n",
        "        gt0    = self.binary_loader(self.gts[index])\n",
        "        depth0 = self.binary_loader(self.depths[index])\n",
        "        \n",
        "        \n",
        "        ##################################################\n",
        "        ## out1\n",
        "        ##################################################\n",
        "        image,gt,depth = cv_random_flip(image0,gt0,depth0)\n",
        "        image,gt,depth = randomCrop(image, gt,depth)\n",
        "        image,gt,depth = randomRotation(image, gt,depth)\n",
        "        image          = colorEnhance(image)\n",
        "        gt             = randomPeper(gt)\n",
        "        image          = self.img_transform(image)\n",
        "        gt             = self.gt_transform(gt)\n",
        "        depth          = self.depths_transform(depth)\n",
        "\n",
        "        ##################################################\n",
        "        ## out1\n",
        "        ##################################################\n",
        "        image2,gt2,depth2 = cv_random_flip(image0,gt0,depth0)\n",
        "        image2,gt2,depth2 = randomCrop(image2, gt2,depth2)\n",
        "        image2,gt2,depth2 = randomRotation(image2, gt2,depth2)\n",
        "        image2          = colorEnhance(image2)\n",
        "        gt2             = randomPeper(gt2)\n",
        "        image2          = self.img_transform(image2)\n",
        "        gt2             = self.gt_transform(gt2)\n",
        "        depth2          = self.depths_transform(depth2)\n",
        "\n",
        "        \n",
        "        return image, gt, depth, image2, gt2, depth2\n",
        "\n",
        "    def filter_files(self):\n",
        "\n",
        "        \n",
        "        assert len(self.images) == len(self.gts) and len(self.gts)==len(self.images)\n",
        "        images = []\n",
        "        gts = []\n",
        "        depths=[]\n",
        "        for img_path, gt_path,depth_path in zip(self.images, self.gts, self.depths):\n",
        "            img = Image.open(img_path)\n",
        "            gt = Image.open(gt_path)\n",
        "            depth= Image.open(depth_path)\n",
        "            if img.size == gt.size and gt.size==depth.size:\n",
        "                images.append(img_path)\n",
        "                gts.append(gt_path)\n",
        "                depths.append(depth_path)\n",
        "        self.images = images\n",
        "        self.gts = gts\n",
        "        self.depths=depths\n",
        "\n",
        "    def rgb_loader(self, path):\n",
        "        with open(path, 'rb') as f:\n",
        "            img = Image.open(f)\n",
        "            return img.convert('RGB')\n",
        "\n",
        "    def binary_loader(self, path):\n",
        "        with open(path, 'rb') as f:\n",
        "            img = Image.open(f)\n",
        "            return img.convert('L')\n",
        "\n",
        "    def resize(self, img, gt, depth):\n",
        "        assert img.size == gt.size and gt.size==depth.size\n",
        "        w, h = img.size\n",
        "        if h < self.trainsize or w < self.trainsize:\n",
        "            h = max(h, self.trainsize)\n",
        "            w = max(w, self.trainsize)\n",
        "            return img.resize((w, h), Image.BILINEAR), gt.resize((w, h), Image.NEAREST),depth.resize((w, h), Image.NEAREST)\n",
        "        else:\n",
        "            return img, gt, depth\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.size\n",
        "\n",
        "\n",
        "\n",
        "class SalObjDataset_var_unlabel(data.Dataset):\n",
        "    def __init__(self, image_root, gt_root,depth_root, trainsize):\n",
        "        \n",
        "        self.trainsize = trainsize\n",
        "        self.images = [image_root + f for f in os.listdir(image_root) if f.endswith('.png')]\n",
        "        self.gts    = [gt_root + f for f in os.listdir(gt_root) if f.endswith('.jpg') or f.endswith('.png')]\n",
        "        self.depths = [depth_root + f for f in os.listdir(depth_root) if f.endswith('.bmp') or f.endswith('.png')]\n",
        "        self.images = sorted(self.images)\n",
        "        self.gts    = sorted(self.gts)\n",
        "        self.depths = sorted(self.depths)\n",
        "        self.filter_files()\n",
        "        self.size   = len(self.images)\n",
        "        \n",
        "        self.img_transform = transforms.Compose([\n",
        "            transforms.Resize((self.trainsize, self.trainsize)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "        self.gt_transform = transforms.Compose([\n",
        "            transforms.Resize((self.trainsize, self.trainsize)),\n",
        "            transforms.ToTensor()])\n",
        "        self.depths_transform = transforms.Compose([transforms.Resize((self.trainsize, self.trainsize)),transforms.ToTensor()])\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \n",
        "        ## read imag, gt, depth\n",
        "        image0 = self.rgb_loader(self.images[index])\n",
        "        gt0    = self.binary_loader(self.gts[index])\n",
        "        depth0 = self.binary_loader(self.depths[index])\n",
        "        \n",
        "        \n",
        "        ##################################################\n",
        "        ## out1\n",
        "        ##################################################\n",
        "        image,gt,depth = cv_random_flip(image0,gt0,depth0)\n",
        "        image,gt,depth = randomCrop(image, gt,depth)\n",
        "        image,gt,depth = randomRotation(image, gt,depth)\n",
        "        image          = colorEnhance(image)\n",
        "        gt             = randomPeper(gt)\n",
        "        image          = self.img_transform(image)\n",
        "        gt             = self.gt_transform(gt)\n",
        "        depth          = self.depths_transform(depth)\n",
        "\n",
        "        ##################################################\n",
        "        ## out1\n",
        "        ##################################################\n",
        "        image2,gt2,depth2 = cv_random_flip(image0,gt0,depth0)\n",
        "        image2,gt2,depth2 = randomCrop(image2, gt2,depth2)\n",
        "        image2,gt2,depth2 = randomRotation(image2, gt2,depth2)\n",
        "        image2          = colorEnhance(image2)\n",
        "        gt2             = randomPeper(gt2)\n",
        "        image2          = self.img_transform(image2)\n",
        "        gt2             = self.gt_transform(gt2)\n",
        "        depth2          = self.depths_transform(depth2)\n",
        "\n",
        "        \n",
        "        return image, gt, depth, image2, gt2, depth2\n",
        "\n",
        "    def filter_files(self):\n",
        "\n",
        "        assert len(self.images) == len(self.gts) and len(self.gts)==len(self.images)\n",
        "        images = []\n",
        "        gts = []\n",
        "        depths=[]\n",
        "        for img_path, gt_path,depth_path in zip(self.images, self.gts, self.depths):\n",
        "            img = Image.open(img_path)\n",
        "            gt = Image.open(gt_path)\n",
        "            depth= Image.open(depth_path)\n",
        "            if img.size == gt.size and gt.size==depth.size:\n",
        "                images.append(img_path)\n",
        "                gts.append(gt_path)\n",
        "                depths.append(depth_path)\n",
        "        self.images = images\n",
        "        self.gts = gts\n",
        "        self.depths=depths\n",
        "\n",
        "    def rgb_loader(self, path):\n",
        "        with open(path, 'rb') as f:\n",
        "            img = Image.open(f)\n",
        "            return img.convert('RGB')\n",
        "\n",
        "    def binary_loader(self, path):\n",
        "        with open(path, 'rb') as f:\n",
        "            img = Image.open(f)\n",
        "            return img.convert('L')\n",
        "\n",
        "    def resize(self, img, gt, depth):\n",
        "        assert img.size == gt.size and gt.size==depth.size\n",
        "        w, h = img.size\n",
        "        if h < self.trainsize or w < self.trainsize:\n",
        "            h = max(h, self.trainsize)\n",
        "            w = max(w, self.trainsize)\n",
        "            return img.resize((w, h), Image.BILINEAR), gt.resize((w, h), Image.NEAREST),depth.resize((w, h), Image.NEAREST)\n",
        "        else:\n",
        "            return img, gt, depth\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.size\n",
        "\n",
        "#dataloader for training\n",
        "def get_loader(image_root, gt_root,depth_root, batchsize, trainsize, shuffle=True, num_workers=12, pin_memory=False):\n",
        "    print(image_root, gt_root, depth_root)\n",
        "    dataset = SalObjDataset(image_root, gt_root, depth_root,trainsize)\n",
        "    print(dataset)\n",
        "    data_loader = data.DataLoader(dataset=dataset,\n",
        "                                  batch_size=batchsize,\n",
        "                                  shuffle=shuffle,\n",
        "                                  num_workers=num_workers,\n",
        "                                  pin_memory=pin_memory)\n",
        "    return data_loader\n",
        "\n",
        "\n",
        "#dataloader for training2\n",
        "## 09-19-2020\n",
        "def get_loader_var(image_root, gt_root,depth_root, batchsize, trainsize, shuffle=True, num_workers=12, pin_memory=False):\n",
        "\n",
        "    dataset = SalObjDataset_var(image_root, gt_root, depth_root,trainsize)\n",
        "    data_loader = data.DataLoader(dataset=dataset,\n",
        "                                  batch_size=batchsize,\n",
        "                                  shuffle=shuffle,\n",
        "                                  num_workers=num_workers,\n",
        "                                  pin_memory=pin_memory)\n",
        "    return data_loader\n",
        "\n",
        "\n",
        "def get_loader_var_unlabel(image_root, gt_root,depth_root, batchsize, trainsize, shuffle=True, num_workers=12, pin_memory=False):\n",
        "\n",
        "    dataset = SalObjDataset_var_unlabel(image_root, gt_root, depth_root,trainsize)\n",
        "    data_loader = data.DataLoader(dataset=dataset,\n",
        "                                  batch_size=batchsize,\n",
        "                                  shuffle=shuffle,\n",
        "                                  num_workers=num_workers,\n",
        "                                  pin_memory=pin_memory)\n",
        "    return data_loader\n",
        "\n",
        "\n",
        "#test dataset and loader\n",
        "class test_dataset:\n",
        "    def __init__(self, image_root, gt_root,depth_root, testsize):\n",
        "        self.testsize = testsize\n",
        "        self.images = [image_root + f for f in os.listdir(image_root) if f.endswith('.jpg') or f.endswith('.png')]\n",
        "        self.gts = [gt_root + f for f in os.listdir(gt_root) if f.endswith('.jpg')\n",
        "                       or f.endswith('.png')]\n",
        "        self.depths=[depth_root + f for f in os.listdir(depth_root) if f.endswith('.bmp')\n",
        "                    or f.endswith('.png')]\n",
        "        self.images = sorted(self.images)\n",
        "        self.gts = sorted(self.gts)\n",
        "        self.depths=sorted(self.depths)\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((self.testsize, self.testsize)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "        self.gt_transform = transforms.ToTensor()\n",
        "        # self.gt_transform = transforms.Compose([\n",
        "        #     transforms.Resize((self.trainsize, self.trainsize)),\n",
        "        #     transforms.ToTensor()])\n",
        "        self.depths_transform = transforms.Compose([transforms.Resize((self.testsize, self.testsize)),transforms.ToTensor()])\n",
        "        self.size = len(self.images)\n",
        "        self.index = 0\n",
        "\n",
        "    def load_data(self):\n",
        "        image = self.rgb_loader(self.images[self.index])\n",
        "        image = self.transform(image).unsqueeze(0)\n",
        "        gt = self.binary_loader(self.gts[self.index])\n",
        "        depth=self.binary_loader(self.depths[self.index])\n",
        "        depth=self.depths_transform(depth).unsqueeze(0)\n",
        "        name = self.images[self.index].split('/')[-1]\n",
        "        image_for_post=self.rgb_loader(self.images[self.index])\n",
        "        image_for_post=image_for_post.resize(gt.size)\n",
        "        if name.endswith('.jpg'):\n",
        "            name = name.split('.jpg')[0] + '.png'\n",
        "        self.index += 1\n",
        "        self.index = self.index % self.size\n",
        "        return image, gt,depth, name,np.array(image_for_post)\n",
        "\n",
        "    def rgb_loader(self, path):\n",
        "        with open(path, 'rb') as f:\n",
        "            img = Image.open(f)\n",
        "            return img.convert('RGB')\n",
        "\n",
        "    def binary_loader(self, path):\n",
        "        with open(path, 'rb') as f:\n",
        "            img = Image.open(f)\n",
        "            return img.convert('L')\n",
        "    def __len__(self):\n",
        "        return self.size\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c5b61bc",
      "metadata": {
        "id": "4c5b61bc"
      },
      "outputs": [],
      "source": [
        "# Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "ea0d700f",
      "metadata": {
        "id": "ea0d700f"
      },
      "outputs": [],
      "source": [
        "from torch.utils import data\n",
        "import torch\n",
        "import os\n",
        "from PIL import Image\n",
        "class EvalDataset(data.Dataset):\n",
        "    def __init__(self, img_root, label_root):\n",
        "        \n",
        "        self.image_path = list(map(lambda x: os.path.join(img_root, x), sorted(os.listdir(img_root))))\n",
        "        self.label_path = list(map(lambda x: os.path.join(label_root, x), sorted(os.listdir(label_root))))\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        pred = Image.open(self.image_path[item]).convert('L')\n",
        "\n",
        "        gt = Image.open(self.label_path[item]).convert('L')\n",
        "        # print(self.image_path[item], self.label_path[item])\n",
        "        if pred.size != gt.size:\n",
        "            pred = pred.resize(gt.size, Image.BILINEAR)\n",
        "        return pred, gt\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_path)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d90242c7",
      "metadata": {
        "id": "d90242c7"
      },
      "outputs": [],
      "source": [
        "#eva_functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "f85ca25c",
      "metadata": {
        "id": "f85ca25c"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Tue Sep 29 17:21:18 2020\n",
        "\n",
        "@author: taozhou\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "## basic funcs\n",
        "###############################################################################\n",
        "\n",
        "def fun_eval_e(y_pred, y, num, cuda=True):\n",
        "    \n",
        "    if cuda:\n",
        "        score = torch.zeros(num).cuda()\n",
        "    else:\n",
        "        score = torch.zeros(num)\n",
        "    \n",
        "    for i in range(num):\n",
        "        \n",
        "        fm = y_pred - y_pred.mean()\n",
        "        gt = y - y.mean()\n",
        "        align_matrix = 2 * gt * fm / (gt * gt + fm * fm + 1e-20)\n",
        "        enhanced = ((align_matrix + 1) * (align_matrix + 1)) / 4\n",
        "        score[i] = torch.sum(enhanced) / (y.numel() - 1 + 1e-20)        \n",
        "    return score.max()\n",
        "\n",
        "\n",
        "def fun_eval_pr(y_pred, y, num, cuda=True):\n",
        "    \n",
        "    if cuda:\n",
        "        prec, recall = torch.zeros(num).cuda(), torch.zeros(num).cuda()\n",
        "        thlist = torch.linspace(0, 1 - 1e-10, num).cuda()\n",
        "    else:\n",
        "        prec, recall = torch.zeros(num), torch.zeros(num)\n",
        "        thlist = torch.linspace(0, 1 - 1e-10, num)\n",
        "    \n",
        "    for i in range(num):\n",
        "        y_temp = (y_pred >= thlist[i]).float()\n",
        "        tp = (y_temp * y).sum()\n",
        "        prec[i], recall[i] = tp / (y_temp.sum() + 1e-20), tp / (y.sum() + 1e-20)\n",
        "    return prec, recall\n",
        "    \n",
        "\n",
        "def fun_S_object(pred, gt):\n",
        "        \n",
        "    fg = torch.where(gt==0, torch.zeros_like(pred), pred)\n",
        "    bg = torch.where(gt==1, torch.zeros_like(pred), 1-pred)\n",
        "    o_fg = fun_object(fg, gt)\n",
        "    o_bg = fun_object(bg, 1-gt)\n",
        "    u = gt.mean()\n",
        "    Q = u * o_fg + (1-u) * o_bg\n",
        "    return Q\n",
        "\n",
        "\n",
        "def fun_object(pred, gt):\n",
        "    \n",
        "    temp = pred[gt == 1]\n",
        "    x = temp.mean()\n",
        "    sigma_x = temp.std()\n",
        "    score = 2.0 * x / (x * x + 1.0 + sigma_x + 1e-20)\n",
        "        \n",
        "    return score\n",
        "\n",
        "\n",
        "def fun_S_region(pred, gt):\n",
        "    \n",
        "    X, Y = fun_centroid(gt)\n",
        "    gt1, gt2, gt3, gt4, w1, w2, w3, w4 = fun_divideGT(gt, X, Y)\n",
        "    p1, p2, p3, p4 = fun_dividePrediction(pred, X, Y)\n",
        "    Q1 = fun_ssim(p1, gt1)\n",
        "    Q2 = fun_ssim(p2, gt2)\n",
        "    Q3 = fun_ssim(p3, gt3)\n",
        "    Q4 = fun_ssim(p4, gt4)\n",
        "    Q = w1*Q1 + w2*Q2 + w3*Q3 + w4*Q4\n",
        "    \n",
        "    return Q\n",
        "    \n",
        "def fun_centroid(gt, cuda=True):\n",
        "    \n",
        "    rows, cols = gt.size()[-2:]\n",
        "    gt = gt.view(rows, cols)\n",
        "    \n",
        "    if gt.sum() == 0:\n",
        "        \n",
        "        if cuda:\n",
        "            X = torch.eye(1).cuda() * round(cols / 2)\n",
        "            Y = torch.eye(1).cuda() * round(rows / 2)\n",
        "        else:\n",
        "            X = torch.eye(1) * round(cols / 2)\n",
        "            Y = torch.eye(1) * round(rows / 2)\n",
        "    \n",
        "    else:\n",
        "        total = gt.sum()\n",
        "        \n",
        "        if cuda:\n",
        "            i = torch.from_numpy(np.arange(0,cols)).cuda().float()\n",
        "            j = torch.from_numpy(np.arange(0,rows)).cuda().float()\n",
        "        else:\n",
        "            i = torch.from_numpy(np.arange(0,cols)).float()\n",
        "            j = torch.from_numpy(np.arange(0,rows)).float()\n",
        "            \n",
        "        X = torch.round((gt.sum(dim=0)*i).sum() / total)\n",
        "        Y = torch.round((gt.sum(dim=1)*j).sum() / total)\n",
        "        \n",
        "    return X.long(), Y.long()\n",
        "  \n",
        "    \n",
        "def fun_divideGT(gt, X, Y):\n",
        "    \n",
        "    h, w = gt.size()[-2:]\n",
        "    area = h*w\n",
        "    gt   = gt.view(h, w)\n",
        "    LT   = gt[:Y, :X]\n",
        "    RT   = gt[:Y, X:w]\n",
        "    LB   = gt[Y:h, :X]\n",
        "    RB   = gt[Y:h, X:w]\n",
        "    X    = X.float()\n",
        "    Y    = Y.float()\n",
        "    w1   = X * Y / area\n",
        "    w2   = (w - X) * Y / area\n",
        "    w3   = X * (h - Y) / area\n",
        "    w4   = 1 - w1 - w2 - w3\n",
        "    \n",
        "    return LT, RT, LB, RB, w1, w2, w3, w4\n",
        "\n",
        "def fun_dividePrediction(pred, X, Y):\n",
        "    \n",
        "    h, w = pred.size()[-2:]\n",
        "    pred = pred.view(h, w)\n",
        "    LT = pred[:Y, :X]\n",
        "    RT = pred[:Y, X:w]\n",
        "    LB = pred[Y:h, :X]\n",
        "    RB = pred[Y:h, X:w]\n",
        "        \n",
        "    return LT, RT, LB, RB\n",
        "\n",
        "\n",
        "def fun_ssim(pred, gt):\n",
        "    \n",
        "    gt       = gt.float()\n",
        "    h, w     = pred.size()[-2:]\n",
        "    N        = h*w\n",
        "    x        = pred.mean()\n",
        "    y        = gt.mean()\n",
        "    sigma_x2 = ((pred - x)*(pred - x)).sum() / (N - 1 + 1e-20)\n",
        "    sigma_y2 = ((gt - y)*(gt - y)).sum() / (N - 1 + 1e-20)\n",
        "    sigma_xy = ((pred - x)*(gt - y)).sum() / (N - 1 + 1e-20)\n",
        "        \n",
        "    aplha = 4 * x * y *sigma_xy\n",
        "    beta = (x*x + y*y) * (sigma_x2 + sigma_y2)\n",
        "    \n",
        "    if aplha != 0:\n",
        "        Q = aplha / (beta + 1e-20)\n",
        "    elif aplha == 0 and beta == 0:\n",
        "        Q = 1.0\n",
        "    else:\n",
        "        Q = 0\n",
        "    \n",
        "    return Q\n",
        "\n",
        "###############################################################################\n",
        "## metric funcs\n",
        "###############################################################################\n",
        "def eval_mae(pred,gt,cuda=True):\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        trans = transforms.Compose([transforms.ToTensor()])\n",
        "        \n",
        "        if cuda:\n",
        "            pred = pred.cuda()\n",
        "            gt   = gt.cuda()\n",
        "#        else:\n",
        "#            pred = trans(pred)\n",
        "#            gt = trans(gt)\n",
        "                \n",
        "        mae = torch.abs(pred - gt).mean()\n",
        "        \n",
        "    return mae.cpu().detach().numpy()\n",
        "                \n",
        "\n",
        "def eval_Smeasure(pred,gt,cuda=True):\n",
        "    \n",
        "    alpha, avg_q, img_num = 0.5, 0.0, 0.0\n",
        "   \n",
        "    with torch.no_grad():\n",
        "        \n",
        "        trans = transforms.Compose([transforms.ToTensor()])\n",
        "        \n",
        "        if cuda:\n",
        "            pred = pred.cuda()\n",
        "            gt   = gt.cuda()\n",
        "\n",
        "        \n",
        "        y = gt.mean()\n",
        "        \n",
        "        ##\n",
        "        if y == 0:\n",
        "            x = pred.mean()\n",
        "            Q = 1.0 - x\n",
        "        elif y == 1:\n",
        "            x = pred.mean()\n",
        "            Q = x\n",
        "        else:\n",
        "            Q = alpha * fun_S_object(pred, gt) + (1-alpha) * fun_S_region(pred, gt)\n",
        "            if Q.item() < 0:\n",
        "                Q = torch.FLoatTensor([0.0])\n",
        "                \n",
        "    return Q.item()\n",
        "\n",
        "                \n",
        "def eval_fmeasure(pred, gt, cuda=True):\n",
        "    print('eval[FMeasure]:{} dataset with {} method.'.format(self.dataset, self.method))\n",
        "    \n",
        "    beta2 = 0.3\n",
        "    avg_p, avg_r, img_num = 0.0, 0.0, 0.0\n",
        "    \n",
        "    ##    \n",
        "    with torch.no_grad():\n",
        "        trans = transforms.Compose([transforms.ToTensor()])\n",
        "        if cuda:\n",
        "            pred = trans(pred).cuda()\n",
        "            gt = trans(gt).cuda()\n",
        "        else:\n",
        "            pred = trans(pred)\n",
        "            gt = trans(gt)\n",
        "                \n",
        "        prec, recall = fun_eval_pr(pred, gt, 255)\n",
        "\n",
        "    return prec, recall\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "            \n",
        "            \n",
        "            \n",
        "            \n",
        "            \n",
        "\n",
        "class Eval_thread():\n",
        "    def __init__(self, loader, method, dataset, output_dir, cuda):\n",
        "        self.loader = loader\n",
        "        self.method = method\n",
        "        self.dataset = dataset\n",
        "        self.cuda = cuda\n",
        "        self.logfile = os.path.join(output_dir, 'result.txt')\n",
        "    def run(self):\n",
        "        start_time = time.time()\n",
        "        mae = self.Eval_mae()\n",
        "        s = self.Eval_Smeasure()\n",
        "        \n",
        "        return mae,s\n",
        "        \n",
        "        #max_f = self.Eval_fmeasure()\n",
        "        #max_e = self.Eval_Emeasure()\n",
        "        \n",
        "        #self.LOG('{} dataset with {} method get {:.4f} mae, {:.4f} max-fmeasure, {:.4f} max-Emeasure, {:.4f} S-measure..\\n'.format(self.dataset, self.method, mae, max_f, max_e, s))\n",
        "        #return '[cost:{:.4f}s]{} dataset with {} method get {:.4f} mae, {:.4f} max-fmeasure, {:.4f} max-Emeasure, {:.4f} S-measure..'.format(time.time()-start_time, self.dataset, self.method, mae, max_f, max_e, s)\n",
        "    \n",
        "    def Eval_mae(self):\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            trans = transforms.Compose([transforms.ToTensor()])\n",
        "            for pred, gt in self.loader:\n",
        "                if self.cuda:\n",
        "                    \n",
        "                    pred = trans(pred).cuda()\n",
        "                    gt = trans(gt).cuda()\n",
        "                else:\n",
        "                    pred = trans(pred)\n",
        "                    gt = trans(gt)\n",
        "                mea = torch.abs(pred - gt).mean()\n",
        "                if mea == mea: # for Nan\n",
        "                    avg_mae += mea\n",
        "                    img_num += 1.0\n",
        "            avg_mae /= img_num\n",
        "            \n",
        "            return avg_mae.item()\n",
        "    \n",
        "    def Eval_fmeasure(self):\n",
        "        print('eval[FMeasure]:{} dataset with {} method.'.format(self.dataset, self.method))\n",
        "        beta2 = 0.3\n",
        "        avg_p, avg_r, img_num = 0.0, 0.0, 0.0\n",
        "        with torch.no_grad():\n",
        "            trans = transforms.Compose([transforms.ToTensor()])\n",
        "            for pred, gt in self.loader:\n",
        "                if self.cuda:\n",
        "                    pred = trans(pred).cuda()\n",
        "                    gt = trans(gt).cuda()\n",
        "                else:\n",
        "                    pred = trans(pred)\n",
        "                    gt = trans(gt)\n",
        "                prec, recall = self._eval_pr(pred, gt, 255)\n",
        "                avg_p += prec\n",
        "                avg_r += recall\n",
        "                img_num += 1.0\n",
        "            avg_p /= img_num\n",
        "            avg_r /= img_num\n",
        "            score = (1 + beta2) * avg_p * avg_r / (beta2 * avg_p + avg_r)\n",
        "            score[score != score] = 0 # for Nan\n",
        "            \n",
        "            return score.max().item()\n",
        "    def Eval_Emeasure(self):\n",
        "        print('eval[EMeasure]:{} dataset with {} method.'.format(self.dataset, self.method))\n",
        "        avg_e, img_num = 0.0, 0.0\n",
        "        with torch.no_grad():\n",
        "            trans = transforms.Compose([transforms.ToTensor()])\n",
        "            for pred, gt in self.loader:\n",
        "                if self.cuda:\n",
        "                    pred = trans(pred).cuda()\n",
        "                    gt = trans(gt).cuda()\n",
        "                else:\n",
        "                    pred = trans(pred)\n",
        "                    gt = trans(gt)\n",
        "                max_e = self._eval_e(pred, gt, 255)\n",
        "                if max_e == max_e:\n",
        "                    avg_e += max_e\n",
        "                    img_num += 1.0\n",
        "                \n",
        "            avg_e /= img_num\n",
        "            return avg_e\n",
        "    def Eval_Smeasure(self):\n",
        "        #print('eval[SMeasure]:{} dataset with {} method.'.format(self.dataset, self.method))\n",
        "        alpha, avg_q, img_num = 0.5, 0.0, 0.0\n",
        "        with torch.no_grad():\n",
        "            trans = transforms.Compose([transforms.ToTensor()])\n",
        "            for pred, gt in self.loader:\n",
        "                if self.cuda:\n",
        "                    pred = trans(pred).cuda()\n",
        "                    gt = trans(gt).cuda()\n",
        "                else:\n",
        "                    pred = trans(pred)\n",
        "                    gt = trans(gt)\n",
        "                y = gt.mean()\n",
        "                if y == 0:\n",
        "                    x = pred.mean()\n",
        "                    Q = 1.0 - x\n",
        "                elif y == 1:\n",
        "                    x = pred.mean()\n",
        "                    Q = x\n",
        "                else:\n",
        "                    Q = alpha * self._S_object(pred, gt) + (1-alpha) * self._S_region(pred, gt)\n",
        "                    if Q.item() < 0:\n",
        "                        Q = torch.FLoatTensor([0.0])\n",
        "                img_num += 1.0\n",
        "                avg_q += Q.item()\n",
        "            avg_q /= img_num\n",
        "            \n",
        "            return avg_q\n",
        "    def LOG(self, output):\n",
        "        with open(self.logfile, 'a') as f:\n",
        "            f.write(output)\n",
        "\n",
        "    def _eval_e(self, y_pred, y, num):\n",
        "        if self.cuda:\n",
        "            score = torch.zeros(num).cuda()\n",
        "        else:\n",
        "            score = torch.zeros(num)\n",
        "        for i in range(num):\n",
        "            fm = y_pred - y_pred.mean()\n",
        "            gt = y - y.mean()\n",
        "            align_matrix = 2 * gt * fm / (gt * gt + fm * fm + 1e-20)\n",
        "            enhanced = ((align_matrix + 1) * (align_matrix + 1)) / 4\n",
        "            score[i] = torch.sum(enhanced) / (y.numel() - 1 + 1e-20)\n",
        "        return score.max()\n",
        "\n",
        "    def _eval_pr(self, y_pred, y, num):\n",
        "        if self.cuda:\n",
        "            prec, recall = torch.zeros(num).cuda(), torch.zeros(num).cuda()\n",
        "            thlist = torch.linspace(0, 1 - 1e-10, num).cuda()\n",
        "        else:\n",
        "            prec, recall = torch.zeros(num), torch.zeros(num)\n",
        "            thlist = torch.linspace(0, 1 - 1e-10, num)\n",
        "        for i in range(num):\n",
        "            y_temp = (y_pred >= thlist[i]).float()\n",
        "            tp = (y_temp * y).sum()\n",
        "            prec[i], recall[i] = tp / (y_temp.sum() + 1e-20), tp / (y.sum() + 1e-20)\n",
        "        return prec, recall\n",
        "    \n",
        "    def _S_object(self, pred, gt):\n",
        "        fg = torch.where(gt==0, torch.zeros_like(pred), pred)\n",
        "        bg = torch.where(gt==1, torch.zeros_like(pred), 1-pred)\n",
        "        o_fg = self._object(fg, gt)\n",
        "        o_bg = self._object(bg, 1-gt)\n",
        "        u = gt.mean()\n",
        "        Q = u * o_fg + (1-u) * o_bg\n",
        "        return Q\n",
        "\n",
        "    def _object(self, pred, gt):\n",
        "        temp = pred[gt == 1]\n",
        "        x = temp.mean()\n",
        "        sigma_x = temp.std()\n",
        "        score = 2.0 * x / (x * x + 1.0 + sigma_x + 1e-20)\n",
        "        \n",
        "        return score\n",
        "\n",
        "    def _S_region(self, pred, gt):\n",
        "        X, Y = self._centroid(gt)\n",
        "        gt1, gt2, gt3, gt4, w1, w2, w3, w4 = self._divideGT(gt, X, Y)\n",
        "        p1, p2, p3, p4 = self._dividePrediction(pred, X, Y)\n",
        "        Q1 = self._ssim(p1, gt1)\n",
        "        Q2 = self._ssim(p2, gt2)\n",
        "        Q3 = self._ssim(p3, gt3)\n",
        "        Q4 = self._ssim(p4, gt4)\n",
        "        Q = w1*Q1 + w2*Q2 + w3*Q3 + w4*Q4\n",
        "        # print(Q)\n",
        "        return Q\n",
        "    \n",
        "    def _centroid(self, gt):\n",
        "        rows, cols = gt.size()[-2:]\n",
        "        gt = gt.view(rows, cols)\n",
        "        if gt.sum() == 0:\n",
        "            if self.cuda:\n",
        "                X = torch.eye(1).cuda() * round(cols / 2)\n",
        "                Y = torch.eye(1).cuda() * round(rows / 2)\n",
        "            else:\n",
        "                X = torch.eye(1) * round(cols / 2)\n",
        "                Y = torch.eye(1) * round(rows / 2)\n",
        "        else:\n",
        "            total = gt.sum()\n",
        "            if self.cuda:\n",
        "                i = torch.from_numpy(np.arange(0,cols)).cuda().float()\n",
        "                j = torch.from_numpy(np.arange(0,rows)).cuda().float()\n",
        "            else:\n",
        "                i = torch.from_numpy(np.arange(0,cols)).float()\n",
        "                j = torch.from_numpy(np.arange(0,rows)).float()\n",
        "            X = torch.round((gt.sum(dim=0)*i).sum() / total)\n",
        "            Y = torch.round((gt.sum(dim=1)*j).sum() / total)\n",
        "        return X.long(), Y.long()\n",
        "    \n",
        "    def _divideGT(self, gt, X, Y):\n",
        "        h, w = gt.size()[-2:]\n",
        "        area = h*w\n",
        "        gt = gt.view(h, w)\n",
        "        LT = gt[:Y, :X]\n",
        "        RT = gt[:Y, X:w]\n",
        "        LB = gt[Y:h, :X]\n",
        "        RB = gt[Y:h, X:w]\n",
        "        X = X.float()\n",
        "        Y = Y.float()\n",
        "        w1 = X * Y / area\n",
        "        w2 = (w - X) * Y / area\n",
        "        w3 = X * (h - Y) / area\n",
        "        w4 = 1 - w1 - w2 - w3\n",
        "        return LT, RT, LB, RB, w1, w2, w3, w4\n",
        "\n",
        "    def _dividePrediction(self, pred, X, Y):\n",
        "        h, w = pred.size()[-2:]\n",
        "        pred = pred.view(h, w)\n",
        "        LT = pred[:Y, :X]\n",
        "        RT = pred[:Y, X:w]\n",
        "        LB = pred[Y:h, :X]\n",
        "        RB = pred[Y:h, X:w]\n",
        "        return LT, RT, LB, RB\n",
        "\n",
        "    def _ssim(self, pred, gt):\n",
        "        gt = gt.float()\n",
        "        h, w = pred.size()[-2:]\n",
        "        N = h*w\n",
        "        x = pred.mean()\n",
        "        y = gt.mean()\n",
        "        sigma_x2 = ((pred - x)*(pred - x)).sum() / (N - 1 + 1e-20)\n",
        "        sigma_y2 = ((gt - y)*(gt - y)).sum() / (N - 1 + 1e-20)\n",
        "        sigma_xy = ((pred - x)*(gt - y)).sum() / (N - 1 + 1e-20)\n",
        "        \n",
        "        aplha = 4 * x * y *sigma_xy\n",
        "        beta = (x*x + y*y) * (sigma_x2 + sigma_y2)\n",
        "\n",
        "        if aplha != 0:\n",
        "            Q = aplha / (beta + 1e-20)\n",
        "        elif aplha == 0 and beta == 0:\n",
        "            Q = 1.0\n",
        "        else:\n",
        "            Q = 0\n",
        "        return Q\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1fa5ceaa",
      "metadata": {
        "id": "1fa5ceaa"
      },
      "outputs": [],
      "source": [
        "#evaluater"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "d8f7b939",
      "metadata": {
        "id": "d8f7b939"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "\n",
        "\n",
        "class Eval_thread():\n",
        "    def __init__(self, loader, method, dataset, output_dir, cuda):\n",
        "        self.loader = loader\n",
        "        self.method = method\n",
        "        self.dataset = dataset\n",
        "        self.cuda = cuda\n",
        "        self.logfile = os.path.join(output_dir, 'result.txt')\n",
        "    def run(self):\n",
        "        start_time = time.time()\n",
        "        mae = self.Eval_mae()\n",
        "        s = self.Eval_Smeasure()\n",
        "        \n",
        "        \n",
        "        \n",
        "        max_f = self.Eval_fmeasure()\n",
        "        max_e = self.Eval_Emeasure()\n",
        "        \n",
        "        return mae,s,max_f,max_e\n",
        "    \n",
        "        \n",
        "        #self.LOG('{} dataset with {} method get {:.4f} mae, {:.4f} max-fmeasure, {:.4f} max-Emeasure, {:.4f} S-measure..\\n'.format(self.dataset, self.method, mae, max_f, max_e, s))\n",
        "        #return '[cost:{:.4f}s]{} dataset with {} method get {:.4f} mae, {:.4f} max-fmeasure, {:.4f} max-Emeasure, {:.4f} S-measure..'.format(time.time()-start_time, self.dataset, self.method, mae, max_f, max_e, s)\n",
        "    \n",
        "    def Eval_mae(self):\n",
        "        #print('eval[MAE]:{} dataset with {} method.'.format(self.dataset, self.method))\n",
        "        avg_mae, img_num = 0.0, 0.0\n",
        "        with torch.no_grad():\n",
        "            trans = transforms.Compose([transforms.ToTensor()])\n",
        "            for pred, gt in self.loader:\n",
        "                if self.cuda:\n",
        "                    pred = trans(pred).cuda()\n",
        "                    gt = trans(gt).cuda()\n",
        "                else:\n",
        "                    pred = trans(pred)\n",
        "                    gt = trans(gt)\n",
        "                mea = torch.abs(pred - gt).mean()\n",
        "                if mea == mea: # for Nan\n",
        "                    avg_mae += mea\n",
        "                    img_num += 1.0\n",
        "            avg_mae /= img_num\n",
        "            \n",
        "            return avg_mae.item()\n",
        "    \n",
        "    def Eval_fmeasure(self):\n",
        "        print('eval[FMeasure]:{} dataset with {} method.'.format(self.dataset, self.method))\n",
        "        beta2 = 0.3\n",
        "        avg_p, avg_r, img_num = 0.0, 0.0, 0.0\n",
        "        with torch.no_grad():\n",
        "            trans = transforms.Compose([transforms.ToTensor()])\n",
        "            for pred, gt in self.loader:\n",
        "                if self.cuda:\n",
        "                    pred = trans(pred).cuda()\n",
        "                    gt = trans(gt).cuda()\n",
        "                else:\n",
        "                    pred = trans(pred)\n",
        "                    gt = trans(gt)\n",
        "                prec, recall = self._eval_pr(pred, gt, 255)\n",
        "                avg_p += prec\n",
        "                avg_r += recall\n",
        "                img_num += 1.0\n",
        "            avg_p /= img_num\n",
        "            avg_r /= img_num\n",
        "            score = (1 + beta2) * avg_p * avg_r / (beta2 * avg_p + avg_r)\n",
        "            score[score != score] = 0 # for Nan\n",
        "            \n",
        "            return score.max().item()\n",
        "    def Eval_Emeasure(self):\n",
        "        print('eval[EMeasure]:{} dataset with {} method.'.format(self.dataset, self.method))\n",
        "        avg_e, img_num = 0.0, 0.0\n",
        "        with torch.no_grad():\n",
        "            trans = transforms.Compose([transforms.ToTensor()])\n",
        "            for pred, gt in self.loader:\n",
        "                if self.cuda:\n",
        "                    pred = trans(pred).cuda()\n",
        "                    gt = trans(gt).cuda()\n",
        "                else:\n",
        "                    pred = trans(pred)\n",
        "                    gt = trans(gt)\n",
        "                max_e = self._eval_e(pred, gt, 255)\n",
        "                if max_e == max_e:\n",
        "                    avg_e += max_e\n",
        "                    img_num += 1.0\n",
        "                \n",
        "            avg_e /= img_num\n",
        "            return avg_e.item()\n",
        "    def Eval_Smeasure(self):\n",
        "        #print('eval[SMeasure]:{} dataset with {} method.'.format(self.dataset, self.method))\n",
        "        alpha, avg_q, img_num = 0.5, 0.0, 0.0\n",
        "        with torch.no_grad():\n",
        "            trans = transforms.Compose([transforms.ToTensor()])\n",
        "            for pred, gt in self.loader:\n",
        "                if self.cuda:\n",
        "                    pred = trans(pred).cuda()\n",
        "                    gt = trans(gt).cuda()\n",
        "                else:\n",
        "                    pred = trans(pred)\n",
        "                    gt = trans(gt)\n",
        "                y = gt.mean()\n",
        "                if y == 0:\n",
        "                    x = pred.mean()\n",
        "                    Q = 1.0 - x\n",
        "                elif y == 1:\n",
        "                    x = pred.mean()\n",
        "                    Q = x\n",
        "                else:\n",
        "                    Q = alpha * self._S_object(pred, gt) + (1-alpha) * self._S_region(pred, gt)\n",
        "                    if Q.item() < 0:\n",
        "                        Q = torch.FLoatTensor([0.0])\n",
        "                img_num += 1.0\n",
        "                avg_q += Q.item()\n",
        "            avg_q /= img_num\n",
        "            \n",
        "            return avg_q\n",
        "    def LOG(self, output):\n",
        "        with open(self.logfile, 'a') as f:\n",
        "            f.write(output)\n",
        "\n",
        "    def _eval_e(self, y_pred, y, num):\n",
        "        if self.cuda:\n",
        "            score = torch.zeros(num).cuda()\n",
        "        else:\n",
        "            score = torch.zeros(num)\n",
        "        for i in range(num):\n",
        "            fm = y_pred - y_pred.mean()\n",
        "            gt = y - y.mean()\n",
        "            align_matrix = 2 * gt * fm / (gt * gt + fm * fm + 1e-20)\n",
        "            enhanced = ((align_matrix + 1) * (align_matrix + 1)) / 4\n",
        "            score[i] = torch.sum(enhanced) / (y.numel() - 1 + 1e-20)\n",
        "        return score.max()\n",
        "\n",
        "    def _eval_pr(self, y_pred, y, num):\n",
        "        if self.cuda:\n",
        "            prec, recall = torch.zeros(num).cuda(), torch.zeros(num).cuda()\n",
        "            thlist = torch.linspace(0, 1 - 1e-10, num).cuda()\n",
        "        else:\n",
        "            prec, recall = torch.zeros(num), torch.zeros(num)\n",
        "            thlist = torch.linspace(0, 1 - 1e-10, num)\n",
        "        for i in range(num):\n",
        "            y_temp = (y_pred >= thlist[i]).float()\n",
        "            tp = (y_temp * y).sum()\n",
        "            prec[i], recall[i] = tp / (y_temp.sum() + 1e-20), tp / (y.sum() + 1e-20)\n",
        "        return prec, recall\n",
        "    \n",
        "    def _S_object(self, pred, gt):\n",
        "        fg = torch.where(gt==0, torch.zeros_like(pred), pred)\n",
        "        bg = torch.where(gt==1, torch.zeros_like(pred), 1-pred)\n",
        "        o_fg = self._object(fg, gt)\n",
        "        o_bg = self._object(bg, 1-gt)\n",
        "        u = gt.mean()\n",
        "        Q = u * o_fg + (1-u) * o_bg\n",
        "        return Q\n",
        "\n",
        "    def _object(self, pred, gt):\n",
        "        temp = pred[gt == 1]\n",
        "        x = temp.mean()\n",
        "        sigma_x = temp.std()\n",
        "        score = 2.0 * x / (x * x + 1.0 + sigma_x + 1e-20)\n",
        "        \n",
        "        return score\n",
        "\n",
        "    def _S_region(self, pred, gt):\n",
        "        X, Y = self._centroid(gt)\n",
        "        gt1, gt2, gt3, gt4, w1, w2, w3, w4 = self._divideGT(gt, X, Y)\n",
        "        p1, p2, p3, p4 = self._dividePrediction(pred, X, Y)\n",
        "        Q1 = self._ssim(p1, gt1)\n",
        "        Q2 = self._ssim(p2, gt2)\n",
        "        Q3 = self._ssim(p3, gt3)\n",
        "        Q4 = self._ssim(p4, gt4)\n",
        "        Q = w1*Q1 + w2*Q2 + w3*Q3 + w4*Q4\n",
        "        # print(Q)\n",
        "        return Q\n",
        "    \n",
        "    def _centroid(self, gt):\n",
        "        rows, cols = gt.size()[-2:]\n",
        "        gt = gt.view(rows, cols)\n",
        "        if gt.sum() == 0:\n",
        "            if self.cuda:\n",
        "                X = torch.eye(1).cuda() * round(cols / 2)\n",
        "                Y = torch.eye(1).cuda() * round(rows / 2)\n",
        "            else:\n",
        "                X = torch.eye(1) * round(cols / 2)\n",
        "                Y = torch.eye(1) * round(rows / 2)\n",
        "        else:\n",
        "            total = gt.sum()\n",
        "            if self.cuda:\n",
        "                i = torch.from_numpy(np.arange(0,cols)).cuda().float()\n",
        "                j = torch.from_numpy(np.arange(0,rows)).cuda().float()\n",
        "            else:\n",
        "                i = torch.from_numpy(np.arange(0,cols)).float()\n",
        "                j = torch.from_numpy(np.arange(0,rows)).float()\n",
        "            X = torch.round((gt.sum(dim=0)*i).sum() / total)\n",
        "            Y = torch.round((gt.sum(dim=1)*j).sum() / total)\n",
        "        return X.long(), Y.long()\n",
        "    \n",
        "    def _divideGT(self, gt, X, Y):\n",
        "        h, w = gt.size()[-2:]\n",
        "        area = h*w\n",
        "        gt = gt.view(h, w)\n",
        "        LT = gt[:Y, :X]\n",
        "        RT = gt[:Y, X:w]\n",
        "        LB = gt[Y:h, :X]\n",
        "        RB = gt[Y:h, X:w]\n",
        "        X = X.float()\n",
        "        Y = Y.float()\n",
        "        w1 = X * Y / area\n",
        "        w2 = (w - X) * Y / area\n",
        "        w3 = X * (h - Y) / area\n",
        "        w4 = 1 - w1 - w2 - w3\n",
        "        return LT, RT, LB, RB, w1, w2, w3, w4\n",
        "\n",
        "    def _dividePrediction(self, pred, X, Y):\n",
        "        h, w = pred.size()[-2:]\n",
        "        pred = pred.view(h, w)\n",
        "        LT = pred[:Y, :X]\n",
        "        RT = pred[:Y, X:w]\n",
        "        LB = pred[Y:h, :X]\n",
        "        RB = pred[Y:h, X:w]\n",
        "        return LT, RT, LB, RB\n",
        "\n",
        "    def _ssim(self, pred, gt):\n",
        "        gt = gt.float()\n",
        "        h, w = pred.size()[-2:]\n",
        "        N = h*w\n",
        "        x = pred.mean()\n",
        "        y = gt.mean()\n",
        "        sigma_x2 = ((pred - x)*(pred - x)).sum() / (N - 1 + 1e-20)\n",
        "        sigma_y2 = ((gt - y)*(gt - y)).sum() / (N - 1 + 1e-20)\n",
        "        sigma_xy = ((pred - x)*(gt - y)).sum() / (N - 1 + 1e-20)\n",
        "        \n",
        "        aplha = 4 * x * y *sigma_xy\n",
        "        beta = (x*x + y*y) * (sigma_x2 + sigma_y2)\n",
        "\n",
        "        if aplha != 0:\n",
        "            Q = aplha / (beta + 1e-20)\n",
        "        elif aplha == 0 and beta == 0:\n",
        "            Q = 1.0\n",
        "        else:\n",
        "            Q = 0\n",
        "        return Q\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfe6b9bc",
      "metadata": {
        "id": "dfe6b9bc"
      },
      "outputs": [],
      "source": [
        "#options"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "8b9aaf3c",
      "metadata": {
        "id": "8b9aaf3c"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "\n",
        "def arguments():\n",
        "  parser = argparse.ArgumentParser()\n",
        "  parser.add_argument('--epoch',       type=int,   default=50,   help='epoch number')\n",
        "  parser.add_argument('--lr',          type=float, default=1e-4,  help='learning rate')\n",
        "  parser.add_argument('--batchsize',   type=int,   default=4,    help='training batch size')\n",
        "  parser.add_argument('--trainsize',   type=int,   default=352,   help='training dataset size')\n",
        "  parser.add_argument('--clip',        type=float, default=0.5,   help='gradient clipping margin')\n",
        "  parser.add_argument('--lw',          type=float, default=0.001, help='weight')\n",
        "  parser.add_argument('--decay_rate',  type=float, default=0.1,   help='decay rate of learning rate')\n",
        "  parser.add_argument('--decay_epoch', type=int,   default=60,    help='every n epochs decay learning rate')\n",
        "  parser.add_argument('--load',        type=str,   default=None,  help='train from checkpoints')\n",
        "  parser.add_argument('--gpu_id',      type=str,   default='0',   help='train use gpu')\n",
        "\n",
        "  parser.add_argument('--rgb_label_root',      type=str, default='/content/tmp/TrainDataset/RGB/',           help='the training rgb images root')\n",
        "  parser.add_argument('--depth_label_root',    type=str, default='/content/tmp/TrainDataset/depth/',         help='the training depth images root')\n",
        "  parser.add_argument('--gt_label_root',       type=str, default='/content/tmp/TrainDataset/GT/',            help='the training gt images root')\n",
        "\n",
        "  parser.add_argument('--val_rgb_root',        type=str, default='/content/tmp/TestDataset1/TestDataset/NJU2K/RGB/',      help='the test rgb images root')\n",
        "  parser.add_argument('--val_depth_root',      type=str, default='/content/tmp/TestDataset1/TestDataset/NJU2K/depth/',    help='the test depth images root')\n",
        "  parser.add_argument('--val_gt_root',         type=str, default='/content/tmp/TestDataset1/TestDataset/NJU2K/GT/',       help='the test gt images root')\n",
        "\n",
        "  parser.add_argument('--save_path',           type=str, default='/content/tmp/',    help='the path to save models and logs')\n",
        "  return parser.parse_args(\"\")\n",
        "\n",
        "opt = arguments()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec6f6e8f",
      "metadata": {
        "id": "ec6f6e8f"
      },
      "outputs": [],
      "source": [
        "#utils "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "ff850acd",
      "metadata": {
        "id": "ff850acd"
      },
      "outputs": [],
      "source": [
        "def clip_gradient(optimizer, grad_clip):\n",
        "    for group in optimizer.param_groups:\n",
        "        for param in group['params']:\n",
        "            if param.grad is not None:\n",
        "                param.grad.data.clamp_(-grad_clip, grad_clip)\n",
        "\n",
        "\n",
        "def adjust_lr(optimizer, init_lr, epoch, decay_rate=0.1, decay_epoch=30):\n",
        "    decay = decay_rate ** (epoch // decay_epoch)\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = decay*init_lr\n",
        "        lr=param_group['lr']\n",
        "    return lr\n",
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        " \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "ce9b3f65",
      "metadata": {
        "id": "ce9b3f65",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08df3d10-4268-4d47-e19a-2f48bfdc9c48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "USE GPU 0\n",
            "load data...\n",
            "/content/tmp/TrainDataset/RGB/ /content/tmp/TrainDataset/GT/ /content/tmp/TrainDataset/depth/\n",
            "/content/tmp/TrainDataset/RGB/ /content/tmp/TrainDataset/GT/ /content/tmp/TrainDataset/depth/\n",
            "SalObjDat\n",
            "SalObjDataset ['/content/tmp/TrainDataset/RGB/RGB_00.png', '/content/tmp/TrainDataset/RGB/RGB_01.png', '/content/tmp/TrainDataset/RGB/RGB_02.png', '/content/tmp/TrainDataset/RGB/RGB_10.png', '/content/tmp/TrainDataset/RGB/RGB_100.png', '/content/tmp/TrainDataset/RGB/RGB_101.png', '/content/tmp/TrainDataset/RGB/RGB_102.png', '/content/tmp/TrainDataset/RGB/RGB_11.png', '/content/tmp/TrainDataset/RGB/RGB_110.png', '/content/tmp/TrainDataset/RGB/RGB_111.png', '/content/tmp/TrainDataset/RGB/RGB_112.png', '/content/tmp/TrainDataset/RGB/RGB_12.png', '/content/tmp/TrainDataset/RGB/RGB_120.png', '/content/tmp/TrainDataset/RGB/RGB_121.png', '/content/tmp/TrainDataset/RGB/RGB_122.png', '/content/tmp/TrainDataset/RGB/RGB_130.png', '/content/tmp/TrainDataset/RGB/RGB_131.png', '/content/tmp/TrainDataset/RGB/RGB_132.png', '/content/tmp/TrainDataset/RGB/RGB_140.png', '/content/tmp/TrainDataset/RGB/RGB_141.png', '/content/tmp/TrainDataset/RGB/RGB_142.png', '/content/tmp/TrainDataset/RGB/RGB_150.png', '/content/tmp/TrainDataset/RGB/RGB_151.png', '/content/tmp/TrainDataset/RGB/RGB_152.png', '/content/tmp/TrainDataset/RGB/RGB_160.png', '/content/tmp/TrainDataset/RGB/RGB_161.png', '/content/tmp/TrainDataset/RGB/RGB_162.png', '/content/tmp/TrainDataset/RGB/RGB_170.png', '/content/tmp/TrainDataset/RGB/RGB_171.png', '/content/tmp/TrainDataset/RGB/RGB_172.png', '/content/tmp/TrainDataset/RGB/RGB_180.png', '/content/tmp/TrainDataset/RGB/RGB_181.png', '/content/tmp/TrainDataset/RGB/RGB_182.png', '/content/tmp/TrainDataset/RGB/RGB_190.png', '/content/tmp/TrainDataset/RGB/RGB_191.png', '/content/tmp/TrainDataset/RGB/RGB_192.png', '/content/tmp/TrainDataset/RGB/RGB_20.png', '/content/tmp/TrainDataset/RGB/RGB_200.png', '/content/tmp/TrainDataset/RGB/RGB_201.png', '/content/tmp/TrainDataset/RGB/RGB_202.png', '/content/tmp/TrainDataset/RGB/RGB_21.png', '/content/tmp/TrainDataset/RGB/RGB_210.png', '/content/tmp/TrainDataset/RGB/RGB_211.png', '/content/tmp/TrainDataset/RGB/RGB_212.png', '/content/tmp/TrainDataset/RGB/RGB_22.png', '/content/tmp/TrainDataset/RGB/RGB_220.png', '/content/tmp/TrainDataset/RGB/RGB_221.png', '/content/tmp/TrainDataset/RGB/RGB_222.png', '/content/tmp/TrainDataset/RGB/RGB_230.png', '/content/tmp/TrainDataset/RGB/RGB_231.png', '/content/tmp/TrainDataset/RGB/RGB_232.png', '/content/tmp/TrainDataset/RGB/RGB_240.png', '/content/tmp/TrainDataset/RGB/RGB_241.png', '/content/tmp/TrainDataset/RGB/RGB_242.png', '/content/tmp/TrainDataset/RGB/RGB_250.png', '/content/tmp/TrainDataset/RGB/RGB_251.png', '/content/tmp/TrainDataset/RGB/RGB_252.png', '/content/tmp/TrainDataset/RGB/RGB_260.png', '/content/tmp/TrainDataset/RGB/RGB_261.png', '/content/tmp/TrainDataset/RGB/RGB_262.png', '/content/tmp/TrainDataset/RGB/RGB_270.png', '/content/tmp/TrainDataset/RGB/RGB_271.png', '/content/tmp/TrainDataset/RGB/RGB_272.png', '/content/tmp/TrainDataset/RGB/RGB_280.png', '/content/tmp/TrainDataset/RGB/RGB_281.png', '/content/tmp/TrainDataset/RGB/RGB_282.png', '/content/tmp/TrainDataset/RGB/RGB_290.png', '/content/tmp/TrainDataset/RGB/RGB_291.png', '/content/tmp/TrainDataset/RGB/RGB_292.png', '/content/tmp/TrainDataset/RGB/RGB_30.png', '/content/tmp/TrainDataset/RGB/RGB_300.png', '/content/tmp/TrainDataset/RGB/RGB_301.png', '/content/tmp/TrainDataset/RGB/RGB_302.png', '/content/tmp/TrainDataset/RGB/RGB_31.png', '/content/tmp/TrainDataset/RGB/RGB_310.png', '/content/tmp/TrainDataset/RGB/RGB_311.png', '/content/tmp/TrainDataset/RGB/RGB_312.png', '/content/tmp/TrainDataset/RGB/RGB_32.png', '/content/tmp/TrainDataset/RGB/RGB_320.png', '/content/tmp/TrainDataset/RGB/RGB_321.png', '/content/tmp/TrainDataset/RGB/RGB_322.png', '/content/tmp/TrainDataset/RGB/RGB_330.png', '/content/tmp/TrainDataset/RGB/RGB_331.png', '/content/tmp/TrainDataset/RGB/RGB_332.png', '/content/tmp/TrainDataset/RGB/RGB_340.png', '/content/tmp/TrainDataset/RGB/RGB_341.png', '/content/tmp/TrainDataset/RGB/RGB_342.png', '/content/tmp/TrainDataset/RGB/RGB_350.png', '/content/tmp/TrainDataset/RGB/RGB_351.png', '/content/tmp/TrainDataset/RGB/RGB_352.png', '/content/tmp/TrainDataset/RGB/RGB_360.png', '/content/tmp/TrainDataset/RGB/RGB_361.png', '/content/tmp/TrainDataset/RGB/RGB_362.png', '/content/tmp/TrainDataset/RGB/RGB_370.png', '/content/tmp/TrainDataset/RGB/RGB_371.png', '/content/tmp/TrainDataset/RGB/RGB_372.png', '/content/tmp/TrainDataset/RGB/RGB_380.png', '/content/tmp/TrainDataset/RGB/RGB_381.png', '/content/tmp/TrainDataset/RGB/RGB_382.png', '/content/tmp/TrainDataset/RGB/RGB_390.png', '/content/tmp/TrainDataset/RGB/RGB_391.png', '/content/tmp/TrainDataset/RGB/RGB_392.png', '/content/tmp/TrainDataset/RGB/RGB_40.png', '/content/tmp/TrainDataset/RGB/RGB_400.png', '/content/tmp/TrainDataset/RGB/RGB_401.png', '/content/tmp/TrainDataset/RGB/RGB_402.png', '/content/tmp/TrainDataset/RGB/RGB_41.png', '/content/tmp/TrainDataset/RGB/RGB_410.png', '/content/tmp/TrainDataset/RGB/RGB_411.png', '/content/tmp/TrainDataset/RGB/RGB_412.png', '/content/tmp/TrainDataset/RGB/RGB_42.png', '/content/tmp/TrainDataset/RGB/RGB_420.png', '/content/tmp/TrainDataset/RGB/RGB_421.png', '/content/tmp/TrainDataset/RGB/RGB_422.png', '/content/tmp/TrainDataset/RGB/RGB_430.png', '/content/tmp/TrainDataset/RGB/RGB_431.png', '/content/tmp/TrainDataset/RGB/RGB_432.png', '/content/tmp/TrainDataset/RGB/RGB_440.png', '/content/tmp/TrainDataset/RGB/RGB_441.png', '/content/tmp/TrainDataset/RGB/RGB_442.png', '/content/tmp/TrainDataset/RGB/RGB_450.png', '/content/tmp/TrainDataset/RGB/RGB_451.png', '/content/tmp/TrainDataset/RGB/RGB_452.png', '/content/tmp/TrainDataset/RGB/RGB_460.png', '/content/tmp/TrainDataset/RGB/RGB_461.png', '/content/tmp/TrainDataset/RGB/RGB_462.png', '/content/tmp/TrainDataset/RGB/RGB_470.png', '/content/tmp/TrainDataset/RGB/RGB_471.png', '/content/tmp/TrainDataset/RGB/RGB_472.png', '/content/tmp/TrainDataset/RGB/RGB_480.png', '/content/tmp/TrainDataset/RGB/RGB_481.png', '/content/tmp/TrainDataset/RGB/RGB_482.png', '/content/tmp/TrainDataset/RGB/RGB_490.png', '/content/tmp/TrainDataset/RGB/RGB_491.png', '/content/tmp/TrainDataset/RGB/RGB_492.png', '/content/tmp/TrainDataset/RGB/RGB_50.png', '/content/tmp/TrainDataset/RGB/RGB_500.png', '/content/tmp/TrainDataset/RGB/RGB_501.png', '/content/tmp/TrainDataset/RGB/RGB_502.png', '/content/tmp/TrainDataset/RGB/RGB_51.png', '/content/tmp/TrainDataset/RGB/RGB_510.png', '/content/tmp/TrainDataset/RGB/RGB_511.png', '/content/tmp/TrainDataset/RGB/RGB_512.png', '/content/tmp/TrainDataset/RGB/RGB_52.png', '/content/tmp/TrainDataset/RGB/RGB_520.png', '/content/tmp/TrainDataset/RGB/RGB_521.png', '/content/tmp/TrainDataset/RGB/RGB_522.png', '/content/tmp/TrainDataset/RGB/RGB_530.png', '/content/tmp/TrainDataset/RGB/RGB_531.png', '/content/tmp/TrainDataset/RGB/RGB_532.png', '/content/tmp/TrainDataset/RGB/RGB_540.png', '/content/tmp/TrainDataset/RGB/RGB_541.png', '/content/tmp/TrainDataset/RGB/RGB_542.png', '/content/tmp/TrainDataset/RGB/RGB_550.png', '/content/tmp/TrainDataset/RGB/RGB_551.png', '/content/tmp/TrainDataset/RGB/RGB_552.png', '/content/tmp/TrainDataset/RGB/RGB_560.png', '/content/tmp/TrainDataset/RGB/RGB_561.png', '/content/tmp/TrainDataset/RGB/RGB_562.png', '/content/tmp/TrainDataset/RGB/RGB_570.png', '/content/tmp/TrainDataset/RGB/RGB_571.png', '/content/tmp/TrainDataset/RGB/RGB_572.png', '/content/tmp/TrainDataset/RGB/RGB_580.png', '/content/tmp/TrainDataset/RGB/RGB_581.png', '/content/tmp/TrainDataset/RGB/RGB_582.png', '/content/tmp/TrainDataset/RGB/RGB_590.png', '/content/tmp/TrainDataset/RGB/RGB_591.png', '/content/tmp/TrainDataset/RGB/RGB_592.png', '/content/tmp/TrainDataset/RGB/RGB_60.png', '/content/tmp/TrainDataset/RGB/RGB_600.png', '/content/tmp/TrainDataset/RGB/RGB_601.png', '/content/tmp/TrainDataset/RGB/RGB_602.png', '/content/tmp/TrainDataset/RGB/RGB_61.png', '/content/tmp/TrainDataset/RGB/RGB_610.png', '/content/tmp/TrainDataset/RGB/RGB_611.png', '/content/tmp/TrainDataset/RGB/RGB_612.png', '/content/tmp/TrainDataset/RGB/RGB_62.png', '/content/tmp/TrainDataset/RGB/RGB_620.png', '/content/tmp/TrainDataset/RGB/RGB_621.png', '/content/tmp/TrainDataset/RGB/RGB_622.png', '/content/tmp/TrainDataset/RGB/RGB_630.png', '/content/tmp/TrainDataset/RGB/RGB_631.png', '/content/tmp/TrainDataset/RGB/RGB_632.png', '/content/tmp/TrainDataset/RGB/RGB_640.png', '/content/tmp/TrainDataset/RGB/RGB_641.png', '/content/tmp/TrainDataset/RGB/RGB_642.png', '/content/tmp/TrainDataset/RGB/RGB_650.png', '/content/tmp/TrainDataset/RGB/RGB_651.png', '/content/tmp/TrainDataset/RGB/RGB_652.png', '/content/tmp/TrainDataset/RGB/RGB_660.png', '/content/tmp/TrainDataset/RGB/RGB_661.png', '/content/tmp/TrainDataset/RGB/RGB_662.png', '/content/tmp/TrainDataset/RGB/RGB_670.png', '/content/tmp/TrainDataset/RGB/RGB_671.png', '/content/tmp/TrainDataset/RGB/RGB_672.png', '/content/tmp/TrainDataset/RGB/RGB_680.png', '/content/tmp/TrainDataset/RGB/RGB_681.png', '/content/tmp/TrainDataset/RGB/RGB_682.png', '/content/tmp/TrainDataset/RGB/RGB_690.png', '/content/tmp/TrainDataset/RGB/RGB_691.png', '/content/tmp/TrainDataset/RGB/RGB_692.png', '/content/tmp/TrainDataset/RGB/RGB_70.png', '/content/tmp/TrainDataset/RGB/RGB_700.png', '/content/tmp/TrainDataset/RGB/RGB_701.png', '/content/tmp/TrainDataset/RGB/RGB_702.png', '/content/tmp/TrainDataset/RGB/RGB_71.png', '/content/tmp/TrainDataset/RGB/RGB_710.png', '/content/tmp/TrainDataset/RGB/RGB_711.png', '/content/tmp/TrainDataset/RGB/RGB_712.png', '/content/tmp/TrainDataset/RGB/RGB_72.png', '/content/tmp/TrainDataset/RGB/RGB_720.png', '/content/tmp/TrainDataset/RGB/RGB_721.png', '/content/tmp/TrainDataset/RGB/RGB_722.png', '/content/tmp/TrainDataset/RGB/RGB_730.png', '/content/tmp/TrainDataset/RGB/RGB_731.png', '/content/tmp/TrainDataset/RGB/RGB_732.png', '/content/tmp/TrainDataset/RGB/RGB_740.png', '/content/tmp/TrainDataset/RGB/RGB_741.png', '/content/tmp/TrainDataset/RGB/RGB_742.png', '/content/tmp/TrainDataset/RGB/RGB_750.png', '/content/tmp/TrainDataset/RGB/RGB_751.png', '/content/tmp/TrainDataset/RGB/RGB_752.png', '/content/tmp/TrainDataset/RGB/RGB_760.png', '/content/tmp/TrainDataset/RGB/RGB_761.png', '/content/tmp/TrainDataset/RGB/RGB_762.png', '/content/tmp/TrainDataset/RGB/RGB_770.png', '/content/tmp/TrainDataset/RGB/RGB_771.png', '/content/tmp/TrainDataset/RGB/RGB_772.png', '/content/tmp/TrainDataset/RGB/RGB_780.png', '/content/tmp/TrainDataset/RGB/RGB_781.png', '/content/tmp/TrainDataset/RGB/RGB_782.png', '/content/tmp/TrainDataset/RGB/RGB_790.png', '/content/tmp/TrainDataset/RGB/RGB_791.png', '/content/tmp/TrainDataset/RGB/RGB_792.png', '/content/tmp/TrainDataset/RGB/RGB_80.png', '/content/tmp/TrainDataset/RGB/RGB_81.png', '/content/tmp/TrainDataset/RGB/RGB_82.png', '/content/tmp/TrainDataset/RGB/RGB_90.png', '/content/tmp/TrainDataset/RGB/RGB_91.png', '/content/tmp/TrainDataset/RGB/RGB_92.png'] ['/content/tmp/TrainDataset/GT/GT_00.png', '/content/tmp/TrainDataset/GT/GT_01.png', '/content/tmp/TrainDataset/GT/GT_02.png', '/content/tmp/TrainDataset/GT/GT_10.png', '/content/tmp/TrainDataset/GT/GT_100.png', '/content/tmp/TrainDataset/GT/GT_101.png', '/content/tmp/TrainDataset/GT/GT_102.png', '/content/tmp/TrainDataset/GT/GT_11.png', '/content/tmp/TrainDataset/GT/GT_110.png', '/content/tmp/TrainDataset/GT/GT_111.png', '/content/tmp/TrainDataset/GT/GT_112.png', '/content/tmp/TrainDataset/GT/GT_12.png', '/content/tmp/TrainDataset/GT/GT_120.png', '/content/tmp/TrainDataset/GT/GT_121.png', '/content/tmp/TrainDataset/GT/GT_122.png', '/content/tmp/TrainDataset/GT/GT_130.png', '/content/tmp/TrainDataset/GT/GT_131.png', '/content/tmp/TrainDataset/GT/GT_132.png', '/content/tmp/TrainDataset/GT/GT_140.png', '/content/tmp/TrainDataset/GT/GT_141.png', '/content/tmp/TrainDataset/GT/GT_142.png', '/content/tmp/TrainDataset/GT/GT_150.png', '/content/tmp/TrainDataset/GT/GT_151.png', '/content/tmp/TrainDataset/GT/GT_152.png', '/content/tmp/TrainDataset/GT/GT_160.png', '/content/tmp/TrainDataset/GT/GT_161.png', '/content/tmp/TrainDataset/GT/GT_162.png', '/content/tmp/TrainDataset/GT/GT_170.png', '/content/tmp/TrainDataset/GT/GT_171.png', '/content/tmp/TrainDataset/GT/GT_172.png', '/content/tmp/TrainDataset/GT/GT_180.png', '/content/tmp/TrainDataset/GT/GT_181.png', '/content/tmp/TrainDataset/GT/GT_182.png', '/content/tmp/TrainDataset/GT/GT_190.png', '/content/tmp/TrainDataset/GT/GT_191.png', '/content/tmp/TrainDataset/GT/GT_192.png', '/content/tmp/TrainDataset/GT/GT_20.png', '/content/tmp/TrainDataset/GT/GT_200.png', '/content/tmp/TrainDataset/GT/GT_201.png', '/content/tmp/TrainDataset/GT/GT_202.png', '/content/tmp/TrainDataset/GT/GT_21.png', '/content/tmp/TrainDataset/GT/GT_210.png', '/content/tmp/TrainDataset/GT/GT_211.png', '/content/tmp/TrainDataset/GT/GT_212.png', '/content/tmp/TrainDataset/GT/GT_22.png', '/content/tmp/TrainDataset/GT/GT_220.png', '/content/tmp/TrainDataset/GT/GT_221.png', '/content/tmp/TrainDataset/GT/GT_222.png', '/content/tmp/TrainDataset/GT/GT_230.png', '/content/tmp/TrainDataset/GT/GT_231.png', '/content/tmp/TrainDataset/GT/GT_232.png', '/content/tmp/TrainDataset/GT/GT_240.png', '/content/tmp/TrainDataset/GT/GT_241.png', '/content/tmp/TrainDataset/GT/GT_242.png', '/content/tmp/TrainDataset/GT/GT_250.png', '/content/tmp/TrainDataset/GT/GT_251.png', '/content/tmp/TrainDataset/GT/GT_252.png', '/content/tmp/TrainDataset/GT/GT_260.png', '/content/tmp/TrainDataset/GT/GT_261.png', '/content/tmp/TrainDataset/GT/GT_262.png', '/content/tmp/TrainDataset/GT/GT_270.png', '/content/tmp/TrainDataset/GT/GT_271.png', '/content/tmp/TrainDataset/GT/GT_272.png', '/content/tmp/TrainDataset/GT/GT_280.png', '/content/tmp/TrainDataset/GT/GT_281.png', '/content/tmp/TrainDataset/GT/GT_282.png', '/content/tmp/TrainDataset/GT/GT_290.png', '/content/tmp/TrainDataset/GT/GT_291.png', '/content/tmp/TrainDataset/GT/GT_292.png', '/content/tmp/TrainDataset/GT/GT_30.png', '/content/tmp/TrainDataset/GT/GT_300.png', '/content/tmp/TrainDataset/GT/GT_301.png', '/content/tmp/TrainDataset/GT/GT_302.png', '/content/tmp/TrainDataset/GT/GT_31.png', '/content/tmp/TrainDataset/GT/GT_310.png', '/content/tmp/TrainDataset/GT/GT_311.png', '/content/tmp/TrainDataset/GT/GT_312.png', '/content/tmp/TrainDataset/GT/GT_32.png', '/content/tmp/TrainDataset/GT/GT_320.png', '/content/tmp/TrainDataset/GT/GT_321.png', '/content/tmp/TrainDataset/GT/GT_322.png', '/content/tmp/TrainDataset/GT/GT_330.png', '/content/tmp/TrainDataset/GT/GT_331.png', '/content/tmp/TrainDataset/GT/GT_332.png', '/content/tmp/TrainDataset/GT/GT_340.png', '/content/tmp/TrainDataset/GT/GT_341.png', '/content/tmp/TrainDataset/GT/GT_342.png', '/content/tmp/TrainDataset/GT/GT_350.png', '/content/tmp/TrainDataset/GT/GT_351.png', '/content/tmp/TrainDataset/GT/GT_352.png', '/content/tmp/TrainDataset/GT/GT_360.png', '/content/tmp/TrainDataset/GT/GT_361.png', '/content/tmp/TrainDataset/GT/GT_362.png', '/content/tmp/TrainDataset/GT/GT_370.png', '/content/tmp/TrainDataset/GT/GT_371.png', '/content/tmp/TrainDataset/GT/GT_372.png', '/content/tmp/TrainDataset/GT/GT_380.png', '/content/tmp/TrainDataset/GT/GT_381.png', '/content/tmp/TrainDataset/GT/GT_382.png', '/content/tmp/TrainDataset/GT/GT_390.png', '/content/tmp/TrainDataset/GT/GT_391.png', '/content/tmp/TrainDataset/GT/GT_392.png', '/content/tmp/TrainDataset/GT/GT_40.png', '/content/tmp/TrainDataset/GT/GT_400.png', '/content/tmp/TrainDataset/GT/GT_401.png', '/content/tmp/TrainDataset/GT/GT_402.png', '/content/tmp/TrainDataset/GT/GT_41.png', '/content/tmp/TrainDataset/GT/GT_410.png', '/content/tmp/TrainDataset/GT/GT_411.png', '/content/tmp/TrainDataset/GT/GT_412.png', '/content/tmp/TrainDataset/GT/GT_42.png', '/content/tmp/TrainDataset/GT/GT_420.png', '/content/tmp/TrainDataset/GT/GT_421.png', '/content/tmp/TrainDataset/GT/GT_422.png', '/content/tmp/TrainDataset/GT/GT_430.png', '/content/tmp/TrainDataset/GT/GT_431.png', '/content/tmp/TrainDataset/GT/GT_432.png', '/content/tmp/TrainDataset/GT/GT_440.png', '/content/tmp/TrainDataset/GT/GT_441.png', '/content/tmp/TrainDataset/GT/GT_442.png', '/content/tmp/TrainDataset/GT/GT_450.png', '/content/tmp/TrainDataset/GT/GT_451.png', '/content/tmp/TrainDataset/GT/GT_452.png', '/content/tmp/TrainDataset/GT/GT_460.png', '/content/tmp/TrainDataset/GT/GT_461.png', '/content/tmp/TrainDataset/GT/GT_462.png', '/content/tmp/TrainDataset/GT/GT_470.png', '/content/tmp/TrainDataset/GT/GT_471.png', '/content/tmp/TrainDataset/GT/GT_472.png', '/content/tmp/TrainDataset/GT/GT_480.png', '/content/tmp/TrainDataset/GT/GT_481.png', '/content/tmp/TrainDataset/GT/GT_482.png', '/content/tmp/TrainDataset/GT/GT_490.png', '/content/tmp/TrainDataset/GT/GT_491.png', '/content/tmp/TrainDataset/GT/GT_492.png', '/content/tmp/TrainDataset/GT/GT_50.png', '/content/tmp/TrainDataset/GT/GT_500.png', '/content/tmp/TrainDataset/GT/GT_501.png', '/content/tmp/TrainDataset/GT/GT_502.png', '/content/tmp/TrainDataset/GT/GT_51.png', '/content/tmp/TrainDataset/GT/GT_510.png', '/content/tmp/TrainDataset/GT/GT_511.png', '/content/tmp/TrainDataset/GT/GT_512.png', '/content/tmp/TrainDataset/GT/GT_52.png', '/content/tmp/TrainDataset/GT/GT_520.png', '/content/tmp/TrainDataset/GT/GT_521.png', '/content/tmp/TrainDataset/GT/GT_522.png', '/content/tmp/TrainDataset/GT/GT_530.png', '/content/tmp/TrainDataset/GT/GT_531.png', '/content/tmp/TrainDataset/GT/GT_532.png', '/content/tmp/TrainDataset/GT/GT_540.png', '/content/tmp/TrainDataset/GT/GT_541.png', '/content/tmp/TrainDataset/GT/GT_542.png', '/content/tmp/TrainDataset/GT/GT_550.png', '/content/tmp/TrainDataset/GT/GT_551.png', '/content/tmp/TrainDataset/GT/GT_552.png', '/content/tmp/TrainDataset/GT/GT_560.png', '/content/tmp/TrainDataset/GT/GT_561.png', '/content/tmp/TrainDataset/GT/GT_562.png', '/content/tmp/TrainDataset/GT/GT_570.png', '/content/tmp/TrainDataset/GT/GT_571.png', '/content/tmp/TrainDataset/GT/GT_572.png', '/content/tmp/TrainDataset/GT/GT_580.png', '/content/tmp/TrainDataset/GT/GT_581.png', '/content/tmp/TrainDataset/GT/GT_582.png', '/content/tmp/TrainDataset/GT/GT_590.png', '/content/tmp/TrainDataset/GT/GT_591.png', '/content/tmp/TrainDataset/GT/GT_592.png', '/content/tmp/TrainDataset/GT/GT_60.png', '/content/tmp/TrainDataset/GT/GT_600.png', '/content/tmp/TrainDataset/GT/GT_601.png', '/content/tmp/TrainDataset/GT/GT_602.png', '/content/tmp/TrainDataset/GT/GT_61.png', '/content/tmp/TrainDataset/GT/GT_610.png', '/content/tmp/TrainDataset/GT/GT_611.png', '/content/tmp/TrainDataset/GT/GT_612.png', '/content/tmp/TrainDataset/GT/GT_62.png', '/content/tmp/TrainDataset/GT/GT_620.png', '/content/tmp/TrainDataset/GT/GT_621.png', '/content/tmp/TrainDataset/GT/GT_622.png', '/content/tmp/TrainDataset/GT/GT_630.png', '/content/tmp/TrainDataset/GT/GT_631.png', '/content/tmp/TrainDataset/GT/GT_632.png', '/content/tmp/TrainDataset/GT/GT_640.png', '/content/tmp/TrainDataset/GT/GT_641.png', '/content/tmp/TrainDataset/GT/GT_642.png', '/content/tmp/TrainDataset/GT/GT_650.png', '/content/tmp/TrainDataset/GT/GT_651.png', '/content/tmp/TrainDataset/GT/GT_652.png', '/content/tmp/TrainDataset/GT/GT_660.png', '/content/tmp/TrainDataset/GT/GT_661.png', '/content/tmp/TrainDataset/GT/GT_662.png', '/content/tmp/TrainDataset/GT/GT_670.png', '/content/tmp/TrainDataset/GT/GT_671.png', '/content/tmp/TrainDataset/GT/GT_672.png', '/content/tmp/TrainDataset/GT/GT_680.png', '/content/tmp/TrainDataset/GT/GT_681.png', '/content/tmp/TrainDataset/GT/GT_682.png', '/content/tmp/TrainDataset/GT/GT_690.png', '/content/tmp/TrainDataset/GT/GT_691.png', '/content/tmp/TrainDataset/GT/GT_692.png', '/content/tmp/TrainDataset/GT/GT_70.png', '/content/tmp/TrainDataset/GT/GT_700.png', '/content/tmp/TrainDataset/GT/GT_701.png', '/content/tmp/TrainDataset/GT/GT_702.png', '/content/tmp/TrainDataset/GT/GT_71.png', '/content/tmp/TrainDataset/GT/GT_710.png', '/content/tmp/TrainDataset/GT/GT_711.png', '/content/tmp/TrainDataset/GT/GT_712.png', '/content/tmp/TrainDataset/GT/GT_72.png', '/content/tmp/TrainDataset/GT/GT_720.png', '/content/tmp/TrainDataset/GT/GT_721.png', '/content/tmp/TrainDataset/GT/GT_722.png', '/content/tmp/TrainDataset/GT/GT_730.png', '/content/tmp/TrainDataset/GT/GT_731.png', '/content/tmp/TrainDataset/GT/GT_732.png', '/content/tmp/TrainDataset/GT/GT_740.png', '/content/tmp/TrainDataset/GT/GT_741.png', '/content/tmp/TrainDataset/GT/GT_742.png', '/content/tmp/TrainDataset/GT/GT_750.png', '/content/tmp/TrainDataset/GT/GT_751.png', '/content/tmp/TrainDataset/GT/GT_752.png', '/content/tmp/TrainDataset/GT/GT_760.png', '/content/tmp/TrainDataset/GT/GT_761.png', '/content/tmp/TrainDataset/GT/GT_762.png', '/content/tmp/TrainDataset/GT/GT_770.png', '/content/tmp/TrainDataset/GT/GT_771.png', '/content/tmp/TrainDataset/GT/GT_772.png', '/content/tmp/TrainDataset/GT/GT_780.png', '/content/tmp/TrainDataset/GT/GT_781.png', '/content/tmp/TrainDataset/GT/GT_782.png', '/content/tmp/TrainDataset/GT/GT_790.png', '/content/tmp/TrainDataset/GT/GT_791.png', '/content/tmp/TrainDataset/GT/GT_792.png', '/content/tmp/TrainDataset/GT/GT_80.png', '/content/tmp/TrainDataset/GT/GT_81.png', '/content/tmp/TrainDataset/GT/GT_82.png', '/content/tmp/TrainDataset/GT/GT_90.png', '/content/tmp/TrainDataset/GT/GT_91.png', '/content/tmp/TrainDataset/GT/GT_92.png']\n",
            "<__main__.SalObjDataset object at 0x7fc6eeb37f50>\n",
            "/content/tmp/TestDataset1/TestDataset/NJU2K/RGB/ /content/tmp/TestDataset1/TestDataset/NJU2K/GT/ /content/tmp/TestDataset1/TestDataset/NJU2K/depth/\n",
            "['/content/tmp/TestDataset1/TestDataset/NJU2K/RGB/RGB_960.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/RGB/RGB_831.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/RGB/RGB_911.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/RGB/RGB_891.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/RGB/RGB_802.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/RGB/RGB_902.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/RGB/RGB_860.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/RGB/RGB_1002.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/RGB/RGB_830.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/RGB/RGB_892.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/RGB/RGB_942.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/RGB/RGB_931.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/RGB/RGB_1001.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/RGB/RGB_820.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/RGB/RGB_840.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/RGB/RGB_961.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/RGB/RGB_980.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/RGB/RGB_801.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/RGB/RGB_851.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/RGB/RGB_832.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/RGB/RGB_971.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/RGB/RGB_842.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/RGB/RGB_951.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/RGB/RGB_972.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/RGB/RGB_981.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/RGB/RGB_850.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/RGB/RGB_811.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/RGB/RGB_880.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/RGB/RGB_810.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/RGB/RGB_930.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/RGB/RGB_982.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/RGB/RGB_950.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/RGB/RGB_941.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/RGB/RGB_861.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/RGB/RGB_922.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/RGB/RGB_910.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/RGB/RGB_970.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/RGB/RGB_812.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/RGB/RGB_841.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/RGB/RGB_921.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/RGB/RGB_900.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/RGB/RGB_920.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/RGB/RGB_912.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/RGB/RGB_1000.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/RGB/RGB_992.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/RGB/RGB_881.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/RGB/RGB_882.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/RGB/RGB_991.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/RGB/RGB_890.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/RGB/RGB_870.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/RGB/RGB_952.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/RGB/RGB_822.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/RGB/RGB_871.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/RGB/RGB_901.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/RGB/RGB_990.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/RGB/RGB_821.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/RGB/RGB_872.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/RGB/RGB_852.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/RGB/RGB_940.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/RGB/RGB_932.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/RGB/RGB_962.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/RGB/RGB_800.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/RGB/RGB_862.png'] ['/content/tmp/TestDataset1/TestDataset/NJU2K/GT/GT_870.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/GT/GT_942.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/GT/GT_811.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/GT/GT_880.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/GT/GT_981.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/GT/GT_892.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/GT/GT_852.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/GT/GT_940.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/GT/GT_830.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/GT/GT_970.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/GT/GT_972.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/GT/GT_952.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/GT/GT_810.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/GT/GT_991.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/GT/GT_900.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/GT/GT_910.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/GT/GT_901.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/GT/GT_890.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/GT/GT_862.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/GT/GT_860.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/GT/GT_841.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/GT/GT_922.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/GT/GT_1002.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/GT/GT_982.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/GT/GT_840.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/GT/GT_822.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/GT/GT_872.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/GT/GT_800.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/GT/GT_932.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/GT/GT_801.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/GT/GT_821.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/GT/GT_832.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/GT/GT_851.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/GT/GT_962.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/GT/GT_802.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/GT/GT_842.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/GT/GT_950.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/GT/GT_891.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/GT/GT_871.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/GT/GT_920.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/GT/GT_930.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/GT/GT_990.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/GT/GT_1001.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/GT/GT_812.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/GT/GT_831.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/GT/GT_960.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/GT/GT_902.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/GT/GT_850.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/GT/GT_971.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/GT/GT_912.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/GT/GT_911.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/GT/GT_921.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/GT/GT_861.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/GT/GT_882.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/GT/GT_961.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/GT/GT_980.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/GT/GT_992.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/GT/GT_820.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/GT/GT_941.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/GT/GT_1000.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/GT/GT_931.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/GT/GT_881.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/GT/GT_951.png'] ['/content/tmp/TestDataset1/TestDataset/NJU2K/depth/depth_860.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/depth/depth_972.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/depth/depth_842.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/depth/depth_810.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/depth/depth_870.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/depth/depth_912.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/depth/depth_831.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/depth/depth_812.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/depth/depth_932.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/depth/depth_840.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/depth/depth_852.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/depth/depth_991.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/depth/depth_910.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/depth/depth_980.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/depth/depth_990.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/depth/depth_970.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/depth/depth_861.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/depth/depth_882.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/depth/depth_832.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/depth/depth_922.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/depth/depth_1002.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/depth/depth_862.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/depth/depth_850.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/depth/depth_930.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/depth/depth_802.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/depth/depth_950.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/depth/depth_820.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/depth/depth_811.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/depth/depth_931.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/depth/depth_982.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/depth/depth_822.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/depth/depth_841.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/depth/depth_992.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/depth/depth_830.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/depth/depth_800.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/depth/depth_871.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/depth/depth_821.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/depth/depth_961.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/depth/depth_851.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/depth/depth_942.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/depth/depth_880.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/depth/depth_940.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/depth/depth_971.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/depth/depth_890.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/depth/depth_900.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/depth/depth_981.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/depth/depth_892.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/depth/depth_911.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/depth/depth_1001.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/depth/depth_1000.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/depth/depth_872.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/depth/depth_941.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/depth/depth_960.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/depth/depth_901.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/depth/depth_962.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/depth/depth_951.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/depth/depth_921.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/depth/depth_891.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/depth/depth_881.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/depth/depth_952.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/depth/depth_920.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/depth/depth_801.png', '/content/tmp/TestDataset1/TestDataset/NJU2K/depth/depth_902.png']\n",
            "ToTensor()\n",
            "60\n",
            "Start train...\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7fc6eeb37310>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 12 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-05-22 20:26:09.694246 Epoch [001/050], Step [0001/0060], Loss1: 1.4762 Loss2: 1.4118 Loss3: 1.4322\n",
            "2022-05-22 20:26:37.950833 Epoch [001/050], Step [0050/0060], Loss1: 1.1990 Loss2: 1.1066 Loss3: 1.0746\n",
            "2022-05-22 20:26:43.441273 Epoch [001/050], Step [0060/0060], Loss1: 1.1905 Loss2: 1.0935 Loss3: 1.0661\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3704: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
            "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 MAE: 0.06764626502990723 ####  bestMAE: 1 bestEpoch: 0\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7fc6eeb37310>\n",
            "2022-05-22 20:26:51.349159 Epoch [002/050], Step [0001/0060], Loss1: 1.1069 Loss2: 1.0039 Loss3: 0.9738\n",
            "2022-05-22 20:27:19.729859 Epoch [002/050], Step [0050/0060], Loss1: 1.1806 Loss2: 1.0868 Loss3: 1.0647\n",
            "2022-05-22 20:27:25.397147 Epoch [002/050], Step [0060/0060], Loss1: 1.1044 Loss2: 1.0207 Loss3: 1.0041\n",
            "Epoch: 2 MAE: 0.07323233559018087 ####  bestMAE: 0.06764626502990723 bestEpoch: 0\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7fc6eeb37310>\n",
            "2022-05-22 20:27:33.486320 Epoch [003/050], Step [0001/0060], Loss1: 1.1421 Loss2: 1.0633 Loss3: 1.0458\n",
            "2022-05-22 20:28:01.153357 Epoch [003/050], Step [0050/0060], Loss1: 1.1194 Loss2: 1.0566 Loss3: 1.0463\n",
            "2022-05-22 20:28:06.854032 Epoch [003/050], Step [0060/0060], Loss1: 1.0141 Loss2: 0.9604 Loss3: 0.9500\n",
            "Epoch: 3 MAE: 0.05961435075790163 ####  bestMAE: 0.06764626502990723 bestEpoch: 0\n",
            "best epoch:3\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7fc6eeb37310>\n",
            "2022-05-22 20:28:16.566038 Epoch [004/050], Step [0001/0060], Loss1: 1.0427 Loss2: 0.9770 Loss3: 0.9662\n",
            "2022-05-22 20:28:44.938870 Epoch [004/050], Step [0050/0060], Loss1: 1.0607 Loss2: 0.9952 Loss3: 0.9846\n",
            "2022-05-22 20:28:50.699857 Epoch [004/050], Step [0060/0060], Loss1: 1.0339 Loss2: 0.9838 Loss3: 0.9783\n",
            "Epoch: 4 MAE: 0.06063198331802609 ####  bestMAE: 0.05961435075790163 bestEpoch: 3\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7fc6eeb37310>\n",
            "2022-05-22 20:28:58.539653 Epoch [005/050], Step [0001/0060], Loss1: 1.0548 Loss2: 1.0019 Loss3: 0.9951\n",
            "2022-05-22 20:29:26.707154 Epoch [005/050], Step [0050/0060], Loss1: 1.0420 Loss2: 1.0041 Loss3: 0.9991\n",
            "2022-05-22 20:29:32.464775 Epoch [005/050], Step [0060/0060], Loss1: 1.1154 Loss2: 1.0632 Loss3: 1.0533\n",
            "Epoch: 5 MAE: 0.06457827855670263 ####  bestMAE: 0.05961435075790163 bestEpoch: 3\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7fc6eeb37310>\n",
            "2022-05-22 20:29:42.363367 Epoch [006/050], Step [0001/0060], Loss1: 1.0401 Loss2: 0.9973 Loss3: 0.9921\n",
            "2022-05-22 20:30:10.786826 Epoch [006/050], Step [0050/0060], Loss1: 1.0798 Loss2: 1.0390 Loss3: 1.0355\n",
            "2022-05-22 20:30:16.571595 Epoch [006/050], Step [0060/0060], Loss1: 0.9846 Loss2: 0.9439 Loss3: 0.9399\n",
            "Epoch: 6 MAE: 0.06290942631070578 ####  bestMAE: 0.05961435075790163 bestEpoch: 3\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7fc6eeb37310>\n",
            "2022-05-22 20:30:24.695693 Epoch [007/050], Step [0001/0060], Loss1: 1.0681 Loss2: 1.0201 Loss3: 1.0149\n",
            "2022-05-22 20:30:52.906742 Epoch [007/050], Step [0050/0060], Loss1: 1.0023 Loss2: 0.9583 Loss3: 0.9528\n",
            "2022-05-22 20:30:58.466382 Epoch [007/050], Step [0060/0060], Loss1: 1.0813 Loss2: 1.0391 Loss3: 1.0344\n",
            "Epoch: 7 MAE: 0.05885909489222937 ####  bestMAE: 0.05961435075790163 bestEpoch: 3\n",
            "best epoch:7\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7fc6eeb37310>\n",
            "2022-05-22 20:31:08.091846 Epoch [008/050], Step [0001/0060], Loss1: 0.9800 Loss2: 0.9578 Loss3: 0.9535\n",
            "2022-05-22 20:31:35.908610 Epoch [008/050], Step [0050/0060], Loss1: 1.0202 Loss2: 0.9845 Loss3: 0.9809\n",
            "2022-05-22 20:31:41.573244 Epoch [008/050], Step [0060/0060], Loss1: 1.0481 Loss2: 1.0202 Loss3: 1.0178\n",
            "Epoch: 8 MAE: 0.05919525116209001 ####  bestMAE: 0.05885909489222937 bestEpoch: 7\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7fc6eeb37310>\n",
            "2022-05-22 20:31:50.081507 Epoch [009/050], Step [0001/0060], Loss1: 1.0488 Loss2: 1.0151 Loss3: 1.0121\n",
            "2022-05-22 20:32:18.496219 Epoch [009/050], Step [0050/0060], Loss1: 0.9726 Loss2: 0.9427 Loss3: 0.9394\n",
            "2022-05-22 20:32:24.263145 Epoch [009/050], Step [0060/0060], Loss1: 0.9702 Loss2: 0.9397 Loss3: 0.9367\n",
            "Epoch: 9 MAE: 0.06492695142352392 ####  bestMAE: 0.05885909489222937 bestEpoch: 7\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7fc6eeb37310>\n",
            "2022-05-22 20:32:32.402913 Epoch [010/050], Step [0001/0060], Loss1: 1.0383 Loss2: 1.0019 Loss3: 0.9961\n",
            "2022-05-22 20:33:00.719647 Epoch [010/050], Step [0050/0060], Loss1: 1.0402 Loss2: 1.0109 Loss3: 1.0076\n",
            "2022-05-22 20:33:06.417955 Epoch [010/050], Step [0060/0060], Loss1: 1.0286 Loss2: 1.0019 Loss3: 0.9994\n",
            "Epoch: 10 MAE: 0.05656798937964062 ####  bestMAE: 0.05885909489222937 bestEpoch: 7\n",
            "best epoch:10\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7fc6eeb37310>\n",
            "2022-05-22 20:33:18.723225 Epoch [011/050], Step [0001/0060], Loss1: 0.9885 Loss2: 0.9660 Loss3: 0.9633\n",
            "2022-05-22 20:33:47.331927 Epoch [011/050], Step [0050/0060], Loss1: 1.0095 Loss2: 0.9854 Loss3: 0.9830\n",
            "2022-05-22 20:33:52.867503 Epoch [011/050], Step [0060/0060], Loss1: 1.0903 Loss2: 1.0581 Loss3: 1.0549\n",
            "Epoch: 11 MAE: 0.05748878827170719 ####  bestMAE: 0.05656798937964062 bestEpoch: 10\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7fc6eeb37310>\n",
            "2022-05-22 20:34:00.848950 Epoch [012/050], Step [0001/0060], Loss1: 1.0644 Loss2: 1.0387 Loss3: 1.0357\n",
            "2022-05-22 20:34:29.425835 Epoch [012/050], Step [0050/0060], Loss1: 0.9873 Loss2: 0.9706 Loss3: 0.9687\n",
            "2022-05-22 20:34:35.277725 Epoch [012/050], Step [0060/0060], Loss1: 0.9869 Loss2: 0.9664 Loss3: 0.9642\n",
            "Epoch: 12 MAE: 0.057423889372083856 ####  bestMAE: 0.05656798937964062 bestEpoch: 10\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7fc6eeb37310>\n",
            "2022-05-22 20:34:43.311177 Epoch [013/050], Step [0001/0060], Loss1: 1.0433 Loss2: 1.0177 Loss3: 1.0154\n",
            "2022-05-22 20:35:11.386002 Epoch [013/050], Step [0050/0060], Loss1: 1.0906 Loss2: 1.0725 Loss3: 1.0694\n",
            "2022-05-22 20:35:17.443851 Epoch [013/050], Step [0060/0060], Loss1: 0.9971 Loss2: 0.9798 Loss3: 0.9775\n",
            "Epoch: 13 MAE: 0.06369671730768112 ####  bestMAE: 0.05656798937964062 bestEpoch: 10\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7fc6eeb37310>\n",
            "2022-05-22 20:35:25.708853 Epoch [014/050], Step [0001/0060], Loss1: 1.0043 Loss2: 0.9918 Loss3: 0.9899\n",
            "2022-05-22 20:35:54.407218 Epoch [014/050], Step [0050/0060], Loss1: 1.0252 Loss2: 1.0023 Loss3: 1.0007\n",
            "2022-05-22 20:36:00.059027 Epoch [014/050], Step [0060/0060], Loss1: 1.0380 Loss2: 1.0185 Loss3: 1.0169\n",
            "Epoch: 14 MAE: 0.06677506991795132 ####  bestMAE: 0.05656798937964062 bestEpoch: 10\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7fc6eeb37310>\n",
            "2022-05-22 20:36:08.128825 Epoch [015/050], Step [0001/0060], Loss1: 1.0424 Loss2: 1.0247 Loss3: 1.0225\n",
            "2022-05-22 20:36:35.944042 Epoch [015/050], Step [0050/0060], Loss1: 1.0746 Loss2: 1.0559 Loss3: 1.0540\n",
            "2022-05-22 20:36:41.700467 Epoch [015/050], Step [0060/0060], Loss1: 0.9690 Loss2: 0.9566 Loss3: 0.9549\n",
            "Epoch: 15 MAE: 0.060919744400751014 ####  bestMAE: 0.05656798937964062 bestEpoch: 10\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7fc6eeb37310>\n",
            "2022-05-22 20:36:51.800534 Epoch [016/050], Step [0001/0060], Loss1: 1.0077 Loss2: 0.9900 Loss3: 0.9881\n",
            "2022-05-22 20:37:20.318929 Epoch [016/050], Step [0050/0060], Loss1: 1.0647 Loss2: 1.0433 Loss3: 1.0411\n",
            "2022-05-22 20:37:26.152579 Epoch [016/050], Step [0060/0060], Loss1: 1.0868 Loss2: 1.0665 Loss3: 1.0642\n",
            "Epoch: 16 MAE: 0.0593876958271814 ####  bestMAE: 0.05656798937964062 bestEpoch: 10\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7fc6eeb37310>\n",
            "2022-05-22 20:37:34.116729 Epoch [017/050], Step [0001/0060], Loss1: 0.9626 Loss2: 0.9528 Loss3: 0.9503\n",
            "2022-05-22 20:38:02.384166 Epoch [017/050], Step [0050/0060], Loss1: 1.0290 Loss2: 1.0138 Loss3: 1.0123\n",
            "2022-05-22 20:38:08.113405 Epoch [017/050], Step [0060/0060], Loss1: 0.9702 Loss2: 0.9542 Loss3: 0.9530\n",
            "Epoch: 17 MAE: 0.06129842243497333 ####  bestMAE: 0.05656798937964062 bestEpoch: 10\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7fc6eeb37310>\n",
            "2022-05-22 20:38:16.566319 Epoch [018/050], Step [0001/0060], Loss1: 1.0241 Loss2: 1.0070 Loss3: 1.0051\n",
            "2022-05-22 20:38:44.424197 Epoch [018/050], Step [0050/0060], Loss1: 1.0188 Loss2: 1.0044 Loss3: 1.0032\n",
            "2022-05-22 20:38:50.232661 Epoch [018/050], Step [0060/0060], Loss1: 1.0050 Loss2: 0.9924 Loss3: 0.9908\n",
            "Epoch: 18 MAE: 0.06327060638912141 ####  bestMAE: 0.05656798937964062 bestEpoch: 10\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7fc6eeb37310>\n",
            "2022-05-22 20:38:58.573954 Epoch [019/050], Step [0001/0060], Loss1: 1.0466 Loss2: 1.0287 Loss3: 1.0277\n",
            "2022-05-22 20:39:27.026831 Epoch [019/050], Step [0050/0060], Loss1: 0.9809 Loss2: 0.9720 Loss3: 0.9700\n",
            "2022-05-22 20:39:32.638211 Epoch [019/050], Step [0060/0060], Loss1: 1.0389 Loss2: 1.0246 Loss3: 1.0229\n",
            "Epoch: 19 MAE: 0.0631086080036466 ####  bestMAE: 0.05656798937964062 bestEpoch: 10\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7fc6eeb37310>\n",
            "2022-05-22 20:39:41.143687 Epoch [020/050], Step [0001/0060], Loss1: 1.0102 Loss2: 0.9940 Loss3: 0.9915\n",
            "2022-05-22 20:40:09.473553 Epoch [020/050], Step [0050/0060], Loss1: 0.9960 Loss2: 0.9826 Loss3: 0.9810\n",
            "2022-05-22 20:40:15.220747 Epoch [020/050], Step [0060/0060], Loss1: 1.0281 Loss2: 1.0191 Loss3: 1.0175\n",
            "Epoch: 20 MAE: 0.06396784494793607 ####  bestMAE: 0.05656798937964062 bestEpoch: 10\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7fc6eeb37310>\n",
            "2022-05-22 20:40:24.806614 Epoch [021/050], Step [0001/0060], Loss1: 0.9961 Loss2: 0.9883 Loss3: 0.9867\n",
            "2022-05-22 20:40:53.565609 Epoch [021/050], Step [0050/0060], Loss1: 1.0209 Loss2: 1.0099 Loss3: 1.0084\n",
            "2022-05-22 20:40:59.406327 Epoch [021/050], Step [0060/0060], Loss1: 1.0602 Loss2: 1.0426 Loss3: 1.0410\n",
            "Epoch: 21 MAE: 0.06089098582192076 ####  bestMAE: 0.05656798937964062 bestEpoch: 10\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7fc6eeb37310>\n",
            "2022-05-22 20:41:07.761272 Epoch [022/050], Step [0001/0060], Loss1: 0.9816 Loss2: 0.9744 Loss3: 0.9732\n",
            "2022-05-22 20:41:36.288837 Epoch [022/050], Step [0050/0060], Loss1: 1.0159 Loss2: 1.0067 Loss3: 1.0055\n",
            "2022-05-22 20:41:42.203020 Epoch [022/050], Step [0060/0060], Loss1: 1.0551 Loss2: 1.0356 Loss3: 1.0342\n",
            "Epoch: 22 MAE: 0.06283041863214402 ####  bestMAE: 0.05656798937964062 bestEpoch: 10\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7fc6eeb37310>\n",
            "2022-05-22 20:41:50.688130 Epoch [023/050], Step [0001/0060], Loss1: 0.9934 Loss2: 0.9819 Loss3: 0.9805\n",
            "2022-05-22 20:42:19.834660 Epoch [023/050], Step [0050/0060], Loss1: 1.0249 Loss2: 1.0147 Loss3: 1.0134\n",
            "2022-05-22 20:42:25.787881 Epoch [023/050], Step [0060/0060], Loss1: 1.0131 Loss2: 1.0006 Loss3: 0.9987\n",
            "Epoch: 23 MAE: 0.06155087016877674 ####  bestMAE: 0.05656798937964062 bestEpoch: 10\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7fc6eeb37310>\n",
            "2022-05-22 20:42:33.909740 Epoch [024/050], Step [0001/0060], Loss1: 0.9964 Loss2: 0.9865 Loss3: 0.9857\n",
            "2022-05-22 20:43:01.808405 Epoch [024/050], Step [0050/0060], Loss1: 1.0170 Loss2: 1.0079 Loss3: 1.0060\n",
            "2022-05-22 20:43:07.495886 Epoch [024/050], Step [0060/0060], Loss1: 1.0074 Loss2: 0.9987 Loss3: 0.9975\n",
            "Epoch: 24 MAE: 0.057993793638925706 ####  bestMAE: 0.05656798937964062 bestEpoch: 10\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7fc6eeb37310>\n",
            "2022-05-22 20:43:15.884069 Epoch [025/050], Step [0001/0060], Loss1: 0.9945 Loss2: 0.9877 Loss3: 0.9866\n",
            "2022-05-22 20:43:44.547373 Epoch [025/050], Step [0050/0060], Loss1: 0.9740 Loss2: 0.9674 Loss3: 0.9662\n",
            "2022-05-22 20:43:50.177747 Epoch [025/050], Step [0060/0060], Loss1: 1.0420 Loss2: 1.0341 Loss3: 1.0330\n",
            "Epoch: 25 MAE: 0.06271446515643408 ####  bestMAE: 0.05656798937964062 bestEpoch: 10\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7fc6eeb37310>\n",
            "2022-05-22 20:44:00.070701 Epoch [026/050], Step [0001/0060], Loss1: 1.0082 Loss2: 0.9958 Loss3: 0.9932\n",
            "2022-05-22 20:44:27.855754 Epoch [026/050], Step [0050/0060], Loss1: 0.9894 Loss2: 0.9773 Loss3: 0.9760\n",
            "2022-05-22 20:44:33.568833 Epoch [026/050], Step [0060/0060], Loss1: 0.9988 Loss2: 0.9916 Loss3: 0.9907\n",
            "Epoch: 26 MAE: 0.0665754923744807 ####  bestMAE: 0.05656798937964062 bestEpoch: 10\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7fc6eeb37310>\n",
            "2022-05-22 20:44:41.798136 Epoch [027/050], Step [0001/0060], Loss1: 0.9577 Loss2: 0.9516 Loss3: 0.9505\n",
            "2022-05-22 20:45:09.569122 Epoch [027/050], Step [0050/0060], Loss1: 0.9995 Loss2: 0.9896 Loss3: 0.9884\n",
            "2022-05-22 20:45:15.099816 Epoch [027/050], Step [0060/0060], Loss1: 1.0028 Loss2: 0.9919 Loss3: 0.9904\n",
            "Epoch: 27 MAE: 0.06511820914253355 ####  bestMAE: 0.05656798937964062 bestEpoch: 10\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7fc6eeb37310>\n",
            "2022-05-22 20:45:23.286002 Epoch [028/050], Step [0001/0060], Loss1: 1.0551 Loss2: 1.0433 Loss3: 1.0421\n",
            "2022-05-22 20:45:51.824790 Epoch [028/050], Step [0050/0060], Loss1: 0.9251 Loss2: 0.9179 Loss3: 0.9164\n",
            "2022-05-22 20:45:57.404772 Epoch [028/050], Step [0060/0060], Loss1: 0.9487 Loss2: 0.9415 Loss3: 0.9402\n",
            "Epoch: 28 MAE: 0.0604571340954493 ####  bestMAE: 0.05656798937964062 bestEpoch: 10\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7fc6eeb37310>\n",
            "2022-05-22 20:46:05.446601 Epoch [029/050], Step [0001/0060], Loss1: 0.9600 Loss2: 0.9515 Loss3: 0.9499\n",
            "2022-05-22 20:46:33.723029 Epoch [029/050], Step [0050/0060], Loss1: 1.0442 Loss2: 1.0350 Loss3: 1.0339\n",
            "2022-05-22 20:46:39.342651 Epoch [029/050], Step [0060/0060], Loss1: 0.9624 Loss2: 0.9567 Loss3: 0.9555\n",
            "Epoch: 29 MAE: 0.06206057775588261 ####  bestMAE: 0.05656798937964062 bestEpoch: 10\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7fc6eeb37310>\n",
            "2022-05-22 20:46:47.738848 Epoch [030/050], Step [0001/0060], Loss1: 1.0078 Loss2: 0.9961 Loss3: 0.9948\n",
            "2022-05-22 20:47:15.724715 Epoch [030/050], Step [0050/0060], Loss1: 1.0134 Loss2: 1.0051 Loss3: 1.0042\n",
            "2022-05-22 20:47:21.651786 Epoch [030/050], Step [0060/0060], Loss1: 1.0536 Loss2: 1.0428 Loss3: 1.0418\n",
            "Epoch: 30 MAE: 0.06606436002822152 ####  bestMAE: 0.05656798937964062 bestEpoch: 10\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7fc6eeb37310>\n",
            "2022-05-22 20:47:31.612772 Epoch [031/050], Step [0001/0060], Loss1: 0.9921 Loss2: 0.9870 Loss3: 0.9861\n",
            "2022-05-22 20:47:59.299537 Epoch [031/050], Step [0050/0060], Loss1: 0.9348 Loss2: 0.9280 Loss3: 0.9268\n",
            "2022-05-22 20:48:04.775528 Epoch [031/050], Step [0060/0060], Loss1: 1.0411 Loss2: 1.0294 Loss3: 1.0285\n",
            "Epoch: 31 MAE: 0.06171518386356416 ####  bestMAE: 0.05656798937964062 bestEpoch: 10\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7fc6eeb37310>\n",
            "2022-05-22 20:48:12.826870 Epoch [032/050], Step [0001/0060], Loss1: 1.0697 Loss2: 1.0607 Loss3: 1.0596\n",
            "2022-05-22 20:48:40.747073 Epoch [032/050], Step [0050/0060], Loss1: 1.0474 Loss2: 1.0366 Loss3: 1.0355\n",
            "2022-05-22 20:48:46.822435 Epoch [032/050], Step [0060/0060], Loss1: 0.9671 Loss2: 0.9609 Loss3: 0.9597\n",
            "Epoch: 32 MAE: 0.06135762971545025 ####  bestMAE: 0.05656798937964062 bestEpoch: 10\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7fc6eeb37310>\n",
            "2022-05-22 20:48:55.194230 Epoch [033/050], Step [0001/0060], Loss1: 0.9855 Loss2: 0.9778 Loss3: 0.9769\n",
            "2022-05-22 20:49:23.422055 Epoch [033/050], Step [0050/0060], Loss1: 1.0531 Loss2: 1.0469 Loss3: 1.0458\n",
            "2022-05-22 20:49:29.113816 Epoch [033/050], Step [0060/0060], Loss1: 0.9366 Loss2: 0.9307 Loss3: 0.9294\n",
            "Epoch: 33 MAE: 0.06291401636032833 ####  bestMAE: 0.05656798937964062 bestEpoch: 10\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7fc6eeb37310>\n",
            "2022-05-22 20:49:37.915148 Epoch [034/050], Step [0001/0060], Loss1: 0.9329 Loss2: 0.9266 Loss3: 0.9254\n",
            "2022-05-22 20:50:06.261542 Epoch [034/050], Step [0050/0060], Loss1: 0.9947 Loss2: 0.9861 Loss3: 0.9848\n",
            "2022-05-22 20:50:11.774388 Epoch [034/050], Step [0060/0060], Loss1: 0.9304 Loss2: 0.9249 Loss3: 0.9237\n",
            "Epoch: 34 MAE: 0.06492472588069853 ####  bestMAE: 0.05656798937964062 bestEpoch: 10\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7fc6eeb37310>\n",
            "2022-05-22 20:50:20.089358 Epoch [035/050], Step [0001/0060], Loss1: 1.0065 Loss2: 1.0005 Loss3: 0.9995\n",
            "2022-05-22 20:50:48.239860 Epoch [035/050], Step [0050/0060], Loss1: 1.0608 Loss2: 1.0539 Loss3: 1.0532\n",
            "2022-05-22 20:50:54.061785 Epoch [035/050], Step [0060/0060], Loss1: 0.9903 Loss2: 0.9810 Loss3: 0.9797\n",
            "Epoch: 35 MAE: 0.06756886981782464 ####  bestMAE: 0.05656798937964062 bestEpoch: 10\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7fc6eeb37310>\n",
            "2022-05-22 20:51:03.968109 Epoch [036/050], Step [0001/0060], Loss1: 0.9439 Loss2: 0.9376 Loss3: 0.9368\n",
            "2022-05-22 20:51:32.815538 Epoch [036/050], Step [0050/0060], Loss1: 0.9213 Loss2: 0.9146 Loss3: 0.9133\n",
            "2022-05-22 20:51:38.421654 Epoch [036/050], Step [0060/0060], Loss1: 0.9503 Loss2: 0.9446 Loss3: 0.9437\n",
            "Epoch: 36 MAE: 0.06207921421717084 ####  bestMAE: 0.05656798937964062 bestEpoch: 10\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7fc6eeb37310>\n",
            "2022-05-22 20:51:46.860698 Epoch [037/050], Step [0001/0060], Loss1: 0.9589 Loss2: 0.9529 Loss3: 0.9519\n",
            "2022-05-22 20:52:15.888531 Epoch [037/050], Step [0050/0060], Loss1: 0.9860 Loss2: 0.9808 Loss3: 0.9797\n",
            "2022-05-22 20:52:21.632015 Epoch [037/050], Step [0060/0060], Loss1: 0.9843 Loss2: 0.9780 Loss3: 0.9771\n",
            "Epoch: 37 MAE: 0.06055532319205149 ####  bestMAE: 0.05656798937964062 bestEpoch: 10\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7fc6eeb37310>\n",
            "2022-05-22 20:52:29.687372 Epoch [038/050], Step [0001/0060], Loss1: 0.9383 Loss2: 0.9332 Loss3: 0.9320\n",
            "2022-05-22 20:52:57.891798 Epoch [038/050], Step [0050/0060], Loss1: 1.0122 Loss2: 1.0060 Loss3: 1.0050\n",
            "2022-05-22 20:53:03.625071 Epoch [038/050], Step [0060/0060], Loss1: 0.9371 Loss2: 0.9321 Loss3: 0.9302\n",
            "Epoch: 38 MAE: 0.06461617833092098 ####  bestMAE: 0.05656798937964062 bestEpoch: 10\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7fc6eeb37310>\n",
            "2022-05-22 20:53:11.665868 Epoch [039/050], Step [0001/0060], Loss1: 0.9704 Loss2: 0.9665 Loss3: 0.9650\n",
            "2022-05-22 20:53:39.820550 Epoch [039/050], Step [0050/0060], Loss1: 1.0364 Loss2: 1.0303 Loss3: 1.0293\n",
            "2022-05-22 20:53:45.648889 Epoch [039/050], Step [0060/0060], Loss1: 0.9990 Loss2: 0.9886 Loss3: 0.9879\n",
            "Epoch: 39 MAE: 0.06508725726415242 ####  bestMAE: 0.05656798937964062 bestEpoch: 10\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7fc6eeb37310>\n",
            "2022-05-22 20:53:53.799007 Epoch [040/050], Step [0001/0060], Loss1: 1.0296 Loss2: 1.0227 Loss3: 1.0215\n",
            "2022-05-22 20:54:22.061594 Epoch [040/050], Step [0050/0060], Loss1: 1.0198 Loss2: 1.0143 Loss3: 1.0135\n",
            "2022-05-22 20:54:27.796732 Epoch [040/050], Step [0060/0060], Loss1: 0.9594 Loss2: 0.9546 Loss3: 0.9535\n",
            "Epoch: 40 MAE: 0.06446159801785903 ####  bestMAE: 0.05656798937964062 bestEpoch: 10\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7fc6eeb37310>\n",
            "2022-05-22 20:54:37.551387 Epoch [041/050], Step [0001/0060], Loss1: 0.9854 Loss2: 0.9787 Loss3: 0.9777\n",
            "2022-05-22 20:55:06.107588 Epoch [041/050], Step [0050/0060], Loss1: 1.0301 Loss2: 1.0200 Loss3: 1.0191\n",
            "2022-05-22 20:55:11.767266 Epoch [041/050], Step [0060/0060], Loss1: 1.0026 Loss2: 0.9961 Loss3: 0.9950\n",
            "Epoch: 41 MAE: 0.06442140261332192 ####  bestMAE: 0.05656798937964062 bestEpoch: 10\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7fc6eeb37310>\n",
            "2022-05-22 20:55:20.072108 Epoch [042/050], Step [0001/0060], Loss1: 1.0305 Loss2: 1.0234 Loss3: 1.0224\n",
            "2022-05-22 20:55:48.316501 Epoch [042/050], Step [0050/0060], Loss1: 0.9456 Loss2: 0.9418 Loss3: 0.9406\n",
            "2022-05-22 20:55:53.905418 Epoch [042/050], Step [0060/0060], Loss1: 0.9998 Loss2: 0.9962 Loss3: 0.9952\n",
            "Epoch: 42 MAE: 0.06031761699252661 ####  bestMAE: 0.05656798937964062 bestEpoch: 10\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7fc6eeb37310>\n",
            "2022-05-22 20:56:01.885736 Epoch [043/050], Step [0001/0060], Loss1: 0.9794 Loss2: 0.9752 Loss3: 0.9742\n",
            "2022-05-22 20:56:30.096579 Epoch [043/050], Step [0050/0060], Loss1: 0.9988 Loss2: 0.9939 Loss3: 0.9928\n",
            "2022-05-22 20:56:35.944602 Epoch [043/050], Step [0060/0060], Loss1: 1.0173 Loss2: 1.0104 Loss3: 1.0092\n",
            "Epoch: 43 MAE: 0.06733098363119457 ####  bestMAE: 0.05656798937964062 bestEpoch: 10\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7fc6eeb37310>\n",
            "2022-05-22 20:56:43.832395 Epoch [044/050], Step [0001/0060], Loss1: 0.9668 Loss2: 0.9623 Loss3: 0.9610\n",
            "2022-05-22 20:57:11.963019 Epoch [044/050], Step [0050/0060], Loss1: 0.9336 Loss2: 0.9282 Loss3: 0.9269\n",
            "2022-05-22 20:57:17.716469 Epoch [044/050], Step [0060/0060], Loss1: 1.0312 Loss2: 1.0263 Loss3: 1.0254\n",
            "Epoch: 44 MAE: 0.0629195163363502 ####  bestMAE: 0.05656798937964062 bestEpoch: 10\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7fc6eeb37310>\n",
            "2022-05-22 20:57:26.172920 Epoch [045/050], Step [0001/0060], Loss1: 0.9886 Loss2: 0.9830 Loss3: 0.9810\n",
            "2022-05-22 20:57:54.075258 Epoch [045/050], Step [0050/0060], Loss1: 1.0089 Loss2: 1.0029 Loss3: 1.0015\n",
            "2022-05-22 20:57:59.581123 Epoch [045/050], Step [0060/0060], Loss1: 1.0309 Loss2: 1.0240 Loss3: 1.0232\n",
            "Epoch: 45 MAE: 0.06454992612202964 ####  bestMAE: 0.05656798937964062 bestEpoch: 10\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7fc6eeb37310>\n",
            "2022-05-22 20:58:09.495824 Epoch [046/050], Step [0001/0060], Loss1: 0.9995 Loss2: 0.9907 Loss3: 0.9898\n",
            "2022-05-22 20:58:37.869057 Epoch [046/050], Step [0050/0060], Loss1: 0.9614 Loss2: 0.9562 Loss3: 0.9551\n",
            "2022-05-22 20:58:43.684690 Epoch [046/050], Step [0060/0060], Loss1: 0.9841 Loss2: 0.9775 Loss3: 0.9764\n",
            "Epoch: 46 MAE: 0.06471680232456753 ####  bestMAE: 0.05656798937964062 bestEpoch: 10\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7fc6eeb37310>\n",
            "2022-05-22 20:58:51.877907 Epoch [047/050], Step [0001/0060], Loss1: 1.0012 Loss2: 0.9953 Loss3: 0.9943\n",
            "2022-05-22 20:59:20.824318 Epoch [047/050], Step [0050/0060], Loss1: 0.9876 Loss2: 0.9808 Loss3: 0.9798\n",
            "2022-05-22 20:59:26.447099 Epoch [047/050], Step [0060/0060], Loss1: 1.0072 Loss2: 1.0009 Loss3: 0.9999\n",
            "Epoch: 47 MAE: 0.0651045154389881 ####  bestMAE: 0.05656798937964062 bestEpoch: 10\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7fc6eeb37310>\n",
            "2022-05-22 20:59:34.521025 Epoch [048/050], Step [0001/0060], Loss1: 1.0584 Loss2: 1.0506 Loss3: 1.0498\n",
            "2022-05-22 21:00:03.090296 Epoch [048/050], Step [0050/0060], Loss1: 0.9652 Loss2: 0.9603 Loss3: 0.9591\n",
            "2022-05-22 21:00:08.698513 Epoch [048/050], Step [0060/0060], Loss1: 1.0361 Loss2: 1.0313 Loss3: 1.0304\n",
            "Epoch: 48 MAE: 0.0634440783848838 ####  bestMAE: 0.05656798937964062 bestEpoch: 10\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7fc6eeb37310>\n",
            "2022-05-22 21:00:16.958128 Epoch [049/050], Step [0001/0060], Loss1: 1.0526 Loss2: 1.0435 Loss3: 1.0427\n",
            "2022-05-22 21:00:45.007387 Epoch [049/050], Step [0050/0060], Loss1: 1.0001 Loss2: 0.9937 Loss3: 0.9927\n",
            "2022-05-22 21:00:50.854056 Epoch [049/050], Step [0060/0060], Loss1: 0.9872 Loss2: 0.9838 Loss3: 0.9825\n",
            "Epoch: 49 MAE: 0.06503339691767615 ####  bestMAE: 0.05656798937964062 bestEpoch: 10\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import sys\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from torchvision.utils import make_grid\n",
        "from tensorboardX import SummaryWriter\n",
        "import logging\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "\n",
        "#set the device for training\n",
        "if opt.gpu_id=='0':\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "    print('USE GPU 0')\n",
        "\n",
        "  \n",
        "cudnn.benchmark = True\n",
        "\n",
        "#build the model\n",
        "model = SPNet(32,50)\n",
        "if(opt.load is not None):\n",
        "    model.load_state_dict(torch.load(opt.load))\n",
        "    print('load model from ',opt.load)\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "if torch.cuda.is_available():  \n",
        "  model.cuda()\n",
        "params    = model.parameters()\n",
        "optimizer = torch.optim.Adam(params, opt.lr)\n",
        "\n",
        "\n",
        "#set the path\n",
        "train_image_root = opt.rgb_label_root\n",
        "train_gt_root    = opt.gt_label_root\n",
        "train_depth_root = opt.depth_label_root\n",
        "\n",
        "val_image_root   = opt.val_rgb_root\n",
        "val_gt_root      = opt.val_gt_root\n",
        "val_depth_root   = opt.val_depth_root\n",
        "save_path        = opt.save_path\n",
        "\n",
        "\n",
        "if not os.path.exists(save_path):\n",
        "    os.makedirs(save_path)\n",
        "\n",
        "#load data\n",
        "print('load data...')\n",
        "print(train_image_root, train_gt_root, train_depth_root)\n",
        "train_loader = get_loader(train_image_root, train_gt_root,train_depth_root, batchsize=opt.batchsize, trainsize=opt.trainsize)\n",
        "test_loader  = test_dataset(val_image_root, val_gt_root,val_depth_root, opt.trainsize)\n",
        "total_step   = len(train_loader)\n",
        "\n",
        "\n",
        "logging.basicConfig(filename=save_path+'log.log',format='[%(asctime)s-%(filename)s-%(levelname)s:%(message)s]', level = logging.INFO,filemode='a',datefmt='%Y-%m-%d %I:%M:%S %p')\n",
        "logging.info(\"BBSNet_unif-Train\")\n",
        "logging.info(\"Config\")\n",
        "logging.info('epoch:{};lr:{};batchsize:{};trainsize:{};clip:{};decay_rate:{};load:{};save_path:{};decay_epoch:{}'.format(opt.epoch,opt.lr,opt.batchsize,opt.trainsize,opt.clip,opt.decay_rate,opt.load,save_path,opt.decay_epoch))\n",
        "\n",
        "#set loss function\n",
        "CE   = torch.nn.BCEWithLogitsLoss()\n",
        "\n",
        "step = 0\n",
        "writer     = SummaryWriter(save_path+'summary')\n",
        "best_mae   = 1\n",
        "best_epoch = 0\n",
        "\n",
        "print(len(train_loader))\n",
        "\n",
        "\n",
        "def structure_loss(pred, mask):\n",
        "    weit  = 1+5*torch.abs(F.avg_pool2d(mask, kernel_size=31, stride=1, padding=15)-mask)\n",
        "    wbce  = F.binary_cross_entropy_with_logits(pred, mask, reduce='none')\n",
        "    wbce  = (weit*wbce).sum(dim=(2,3))/weit.sum(dim=(2,3))\n",
        "\n",
        "    pred  = torch.sigmoid(pred)\n",
        "    inter = ((pred*mask)*weit).sum(dim=(2,3))\n",
        "    union = ((pred+mask)*weit).sum(dim=(2,3))\n",
        "    wiou  = 1-(inter+1)/(union-inter+1)\n",
        "    return (wbce+wiou).mean()\n",
        "\n",
        "\n",
        "\n",
        "def train(train_loader, model, optimizer, epoch,save_path):\n",
        "    global step\n",
        "    model.train()\n",
        "    loss_all=0\n",
        "    epoch_step=0\n",
        "    try:\n",
        "        for i, (images, gts, depths) in enumerate(train_loader, start=1):\n",
        "            optimizer.zero_grad()\n",
        "            if torch.cuda.is_available():\n",
        "              images   = images.cuda()\n",
        "              gts      = gts.cuda()\n",
        "              depths   = depths.cuda()\n",
        "\n",
        "            ##\n",
        "            pre_res  = model(images,depths)\n",
        "            \n",
        "            loss1    = structure_loss(pre_res[0], gts) \n",
        "            loss2    = structure_loss(pre_res[1], gts)\n",
        "            loss3    = structure_loss(pre_res[2], gts) \n",
        "            \n",
        "            loss_seg = loss1 + loss2 + loss3\n",
        "\n",
        "            loss = loss_seg \n",
        "            loss.backward()\n",
        "\n",
        "            clip_gradient(optimizer, opt.clip)\n",
        "            optimizer.step()\n",
        "            step+=1\n",
        "            epoch_step+=1\n",
        "            loss_all+=loss.data\n",
        "            if i % 50 == 0 or i == total_step or i==1:\n",
        "                print('{} Epoch [{:03d}/{:03d}], Step [{:04d}/{:04d}], Loss1: {:.4f} Loss2: {:0.4f} Loss3: {:0.4f}'.\n",
        "                    format(datetime.now(), epoch, opt.epoch, i, total_step, loss1.data, loss2.data,  loss3.data))\n",
        "                logging.info('#TRAIN#:Epoch [{:03d}/{:03d}], Step [{:04d}/{:04d}], Loss1: {:.4f} Loss2: {:0.4f} Loss3: {:0.4f}'.\n",
        "                    format( epoch, opt.epoch, i, total_step, loss1.data, loss2.data, loss3.data))\n",
        "                \n",
        "        loss_all/=epoch_step\n",
        "        logging.info('#TRAIN#:Epoch [{:03d}/{:03d}], Loss_AVG: {:.4f}'.format( epoch, opt.epoch, loss_all))\n",
        "        writer.add_scalar('Loss-epoch', loss_all, global_step=epoch)\n",
        "        \n",
        "        if (epoch) % 5 == 0:\n",
        "            torch.save(model.state_dict(), save_path+'HyperNet_epoch_{}.pth'.format(epoch))\n",
        "            \n",
        "    except KeyboardInterrupt: \n",
        "        print('Keyboard Interrupt: save model and exit.')\n",
        "        if not os.path.exists(save_path):\n",
        "            os.makedirs(save_path)\n",
        "        torch.save(model.state_dict(), save_path+'HyperNet_epoch_{}.pth'.format(epoch+1))\n",
        "        print('save checkpoints successfully!')\n",
        "        raise\n",
        "        \n",
        "        \n",
        "        \n",
        "#test function\n",
        "def val(test_loader,model,epoch,save_path):\n",
        "    global best_mae,best_epoch\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        mae_sum=0\n",
        "        for i in range(test_loader.size):\n",
        "            image, gt,depth, name,img_for_post = test_loader.load_data()\n",
        "            gt      = np.asarray(gt, np.float32)\n",
        "            gt     /= (gt.max() + 1e-8)\n",
        "            if torch.cuda.is_available():\n",
        "              image   = image.cuda()\n",
        "              depth   = depth.cuda()\n",
        "            pre_res = model(image,depth)\n",
        "            res     = pre_res[2]\n",
        "            res     = F.upsample(res, size=gt.shape, mode='bilinear', align_corners=False)\n",
        "            res     = res.sigmoid().data.cpu().numpy().squeeze()\n",
        "            res     = (res - res.min()) / (res.max() - res.min() + 1e-8)\n",
        "            mae_sum += np.sum(np.abs(res-gt))*1.0/(gt.shape[0]*gt.shape[1])\n",
        "        #to prevent zero_division error\n",
        "        if test_loader.size == 0:\n",
        "          mae = test_loader.size\n",
        "        else:    \n",
        "          mae = mae_sum/test_loader.size\n",
        "        writer.add_scalar('MAE', torch.tensor(mae), global_step=epoch)\n",
        "        print('Epoch: {} MAE: {} ####  bestMAE: {} bestEpoch: {}'.format(epoch,mae,best_mae,best_epoch))\n",
        "        if epoch==1:\n",
        "            best_mae = mae\n",
        "        else:\n",
        "            if mae<best_mae:\n",
        "                best_mae   = mae\n",
        "                best_epoch = epoch\n",
        "                torch.save(model.state_dict(), save_path+'SPNet_epoch_best.pth')\n",
        "                print('best epoch:{}'.format(epoch))\n",
        "                \n",
        "        logging.info('#TEST#:Epoch:{} MAE:{} bestEpoch:{} bestMAE:{}'.format(epoch,mae,best_epoch,best_mae))\n",
        " \n",
        "if __name__ == '__main__':\n",
        "    print(\"Start train...\")\n",
        "    \n",
        "    for epoch in range(1, opt.epoch):\n",
        "        \n",
        "        cur_lr = adjust_lr(optimizer, opt.lr, epoch, opt.decay_rate, opt.decay_epoch)\n",
        "        writer.add_scalar('learning_rate', cur_lr, global_step=epoch)\n",
        "        # train\n",
        "        print(train_loader)\n",
        "        train(train_loader, model, optimizer, epoch,save_path)\n",
        "        \n",
        "        #test\n",
        "        val(test_loader,model,epoch,save_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import sys\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import os, argparse\n",
        "import cv2\n",
        "\n",
        "def test_arguments():\n",
        "  parser = argparse.ArgumentParser()\n",
        "  parser.add_argument('--testsize', type=int, default=352, help='testing size')\n",
        "  parser.add_argument('--gpu_id',   type=str, default='0', help='select gpu id')\n",
        "  parser.add_argument('--test_path',type=str, default='/content/tmp/TestDataset1/TestDataset/',help='test dataset path')\n",
        "  return parser.parse_args(\"\")\n",
        "\n",
        "opt = test_arguments()\n",
        "\n",
        "dataset_path = opt.test_path\n",
        "\n",
        "#set device for test\n",
        "if opt.gpu_id=='0':\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "    print('USE GPU 0')\n",
        " \n",
        "\n",
        "#load the model\n",
        "model = SPNet(32,50)\n",
        "model.cuda()\n",
        "\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/Checkpoint/SPNet/SPNet_model_best.pth'))\n",
        "model.eval()\n",
        "\n",
        "#test\n",
        "test_datasets = ['NJU2K','NLPR', 'DES'] \n",
        "\n",
        "test_datasets = ['DES'] \n",
        "\n",
        "\n",
        "for dataset in test_datasets:\n",
        "    save_path = '/content/drive/MyDrive/test_maps/SPNet/' + dataset + '/'\n",
        "    if not os.path.exists(save_path):\n",
        "        os.makedirs(save_path)\n",
        "        \n",
        "    image_root  = str(dataset_path + dataset + '/RGB/')\n",
        "    gt_root     = str(dataset_path + dataset + '/GT/')\n",
        "    depth_root  = str(dataset_path + dataset + '/depth/')\n",
        "    test_loader = test_dataset(image_root, gt_root,depth_root, opt.testsize)\n",
        "    for i in range(test_loader.size):\n",
        "        image, gt,depth, name, image_for_post = test_loader.load_data()\n",
        "        gt      = np.asarray(gt, np.float32)\n",
        "        gt     /= (gt.max() + 1e-8)\n",
        "        image   = image.cuda()\n",
        "        depth   = depth.cuda()\n",
        "        pre_res = model(image,depth)\n",
        "        res     = pre_res[2]     \n",
        "        res     = F.upsample(res, size=gt.shape, mode='bilinear', align_corners=False)\n",
        "        res     = res.sigmoid().data.cpu().numpy().squeeze()\n",
        "        res     = (res - res.min()) / (res.max() - res.min() + 1e-8)\n",
        "        \n",
        "        print('save img to: ',save_path+name)\n",
        "        cv2.imwrite(save_path+name,res*255)\n",
        "    print('Test Done!')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4uiefTtiS1o6",
        "outputId": "5d23b9e6-cef3-4636-b647-55f2bc5ca509"
      },
      "id": "4uiefTtiS1o6",
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "USE GPU 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3704: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
            "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "save img to:  /content/drive/MyDrive/test_maps/SPNet/DES/RGB_1500.png\n",
            "save img to:  /content/drive/MyDrive/test_maps/SPNet/DES/RGB_1501.png\n",
            "save img to:  /content/drive/MyDrive/test_maps/SPNet/DES/RGB_1502.png\n",
            "save img to:  /content/drive/MyDrive/test_maps/SPNet/DES/RGB_1510.png\n",
            "save img to:  /content/drive/MyDrive/test_maps/SPNet/DES/RGB_1511.png\n",
            "save img to:  /content/drive/MyDrive/test_maps/SPNet/DES/RGB_1512.png\n",
            "save img to:  /content/drive/MyDrive/test_maps/SPNet/DES/RGB_1520.png\n",
            "save img to:  /content/drive/MyDrive/test_maps/SPNet/DES/RGB_1521.png\n",
            "save img to:  /content/drive/MyDrive/test_maps/SPNet/DES/RGB_1522.png\n",
            "save img to:  /content/drive/MyDrive/test_maps/SPNet/DES/RGB_1530.png\n",
            "save img to:  /content/drive/MyDrive/test_maps/SPNet/DES/RGB_1531.png\n",
            "save img to:  /content/drive/MyDrive/test_maps/SPNet/DES/RGB_1532.png\n",
            "save img to:  /content/drive/MyDrive/test_maps/SPNet/DES/RGB_1540.png\n",
            "save img to:  /content/drive/MyDrive/test_maps/SPNet/DES/RGB_1541.png\n",
            "save img to:  /content/drive/MyDrive/test_maps/SPNet/DES/RGB_1542.png\n",
            "save img to:  /content/drive/MyDrive/test_maps/SPNet/DES/RGB_1550.png\n",
            "save img to:  /content/drive/MyDrive/test_maps/SPNet/DES/RGB_1551.png\n",
            "save img to:  /content/drive/MyDrive/test_maps/SPNet/DES/RGB_1552.png\n",
            "save img to:  /content/drive/MyDrive/test_maps/SPNet/DES/RGB_1560.png\n",
            "save img to:  /content/drive/MyDrive/test_maps/SPNet/DES/RGB_1561.png\n",
            "save img to:  /content/drive/MyDrive/test_maps/SPNet/DES/RGB_1562.png\n",
            "save img to:  /content/drive/MyDrive/test_maps/SPNet/DES/RGB_1570.png\n",
            "save img to:  /content/drive/MyDrive/test_maps/SPNet/DES/RGB_1571.png\n",
            "save img to:  /content/drive/MyDrive/test_maps/SPNet/DES/RGB_1572.png\n",
            "save img to:  /content/drive/MyDrive/test_maps/SPNet/DES/RGB_1580.png\n",
            "save img to:  /content/drive/MyDrive/test_maps/SPNet/DES/RGB_1581.png\n",
            "save img to:  /content/drive/MyDrive/test_maps/SPNet/DES/RGB_1582.png\n",
            "save img to:  /content/drive/MyDrive/test_maps/SPNet/DES/RGB_1590.png\n",
            "save img to:  /content/drive/MyDrive/test_maps/SPNet/DES/RGB_1591.png\n",
            "save img to:  /content/drive/MyDrive/test_maps/SPNet/DES/RGB_1592.png\n",
            "save img to:  /content/drive/MyDrive/test_maps/SPNet/DES/RGB_1600.png\n",
            "save img to:  /content/drive/MyDrive/test_maps/SPNet/DES/RGB_1601.png\n",
            "save img to:  /content/drive/MyDrive/test_maps/SPNet/DES/RGB_1602.png\n",
            "save img to:  /content/drive/MyDrive/test_maps/SPNet/DES/RGB_1610.png\n",
            "save img to:  /content/drive/MyDrive/test_maps/SPNet/DES/RGB_1611.png\n",
            "save img to:  /content/drive/MyDrive/test_maps/SPNet/DES/RGB_1612.png\n",
            "save img to:  /content/drive/MyDrive/test_maps/SPNet/DES/RGB_1620.png\n",
            "save img to:  /content/drive/MyDrive/test_maps/SPNet/DES/RGB_1621.png\n",
            "save img to:  /content/drive/MyDrive/test_maps/SPNet/DES/RGB_1622.png\n",
            "save img to:  /content/drive/MyDrive/test_maps/SPNet/DES/RGB_1630.png\n",
            "save img to:  /content/drive/MyDrive/test_maps/SPNet/DES/RGB_1631.png\n",
            "save img to:  /content/drive/MyDrive/test_maps/SPNet/DES/RGB_1632.png\n",
            "save img to:  /content/drive/MyDrive/test_maps/SPNet/DES/RGB_1640.png\n",
            "save img to:  /content/drive/MyDrive/test_maps/SPNet/DES/RGB_1641.png\n",
            "save img to:  /content/drive/MyDrive/test_maps/SPNet/DES/RGB_1642.png\n",
            "save img to:  /content/drive/MyDrive/test_maps/SPNet/DES/RGB_1650.png\n",
            "save img to:  /content/drive/MyDrive/test_maps/SPNet/DES/RGB_1651.png\n",
            "save img to:  /content/drive/MyDrive/test_maps/SPNet/DES/RGB_1652.png\n",
            "save img to:  /content/drive/MyDrive/test_maps/SPNet/DES/RGB_1660.png\n",
            "save img to:  /content/drive/MyDrive/test_maps/SPNet/DES/RGB_1661.png\n",
            "save img to:  /content/drive/MyDrive/test_maps/SPNet/DES/RGB_1662.png\n",
            "save img to:  /content/drive/MyDrive/test_maps/SPNet/DES/RGB_1670.png\n",
            "save img to:  /content/drive/MyDrive/test_maps/SPNet/DES/RGB_1671.png\n",
            "save img to:  /content/drive/MyDrive/test_maps/SPNet/DES/RGB_1672.png\n",
            "save img to:  /content/drive/MyDrive/test_maps/SPNet/DES/RGB_1680.png\n",
            "save img to:  /content/drive/MyDrive/test_maps/SPNet/DES/RGB_1681.png\n",
            "save img to:  /content/drive/MyDrive/test_maps/SPNet/DES/RGB_1682.png\n",
            "save img to:  /content/drive/MyDrive/test_maps/SPNet/DES/RGB_1690.png\n",
            "save img to:  /content/drive/MyDrive/test_maps/SPNet/DES/RGB_1691.png\n",
            "save img to:  /content/drive/MyDrive/test_maps/SPNet/DES/RGB_1692.png\n",
            "save img to:  /content/drive/MyDrive/test_maps/SPNet/DES/RGB_1700.png\n",
            "save img to:  /content/drive/MyDrive/test_maps/SPNet/DES/RGB_1701.png\n",
            "save img to:  /content/drive/MyDrive/test_maps/SPNet/DES/RGB_1702.png\n",
            "Test Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import argparse\n",
        "import os.path as osp\n",
        "import os\n",
        "import scipy.io as scio \n",
        "# from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "\n",
        "def main(cfg):\n",
        "    \n",
        "    root_dir = cfg.root_dir\n",
        "    gt_dir   = cfg.gt_dir\n",
        "    \n",
        "    if cfg.save_dir is not None:\n",
        "        output_dir = cfg.save_dir\n",
        "    else:\n",
        "        output_dir = root_dir\n",
        "        \n",
        "        \n",
        "    method_names  = cfg.methods\n",
        "    dataset_names = cfg.datasets\n",
        "        \n",
        "    \n",
        "#    if cfg.methods is None:\n",
        "#        method_names = os.listdir(pred_dir)\n",
        "#    else:\n",
        "#        method_names = cfg.methods.split(' ')\n",
        "#    if cfg.datasets is None:\n",
        "#        dataset_names = os.listdir(gt_dir)\n",
        "#    else:\n",
        "#        dataset_names = cfg.datasets.split(' ')\n",
        "    \n",
        "    threads = []\n",
        "    \n",
        "    for method in method_names:\n",
        "        \n",
        "        test_res = []\n",
        "        \n",
        "        for dataset in dataset_names:\n",
        "            loader = EvalDataset(osp.join(root_dir, method, dataset), osp.join(gt_dir, dataset,'GT'))\n",
        "            thread = Eval_thread(loader, method, dataset, output_dir, cfg.cuda)\n",
        "            threads.append(thread)\n",
        "\n",
        "            ##\n",
        "            print(['Evaluating----------',dataset,'----------'])\n",
        "            mae,s,max_f,max_e= thread.run()    ## only compute MAE and s_measure\n",
        "            \n",
        "            print(['MAE:',mae,'----- Smeansure:',s,'----- max_f:',max_f,'----- max_e:',max_e])\n",
        "            \n",
        "            test_res.append([mae,s,max_f,max_e])\n",
        "            scio.savemat('res.mat', {'test_res':test_res})  \n",
        "            \n",
        "            \n",
        "            \n",
        "            \n",
        "            \n",
        "#            \n",
        "#    for thread in threads:\n",
        "#        print(thread.run())\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "    \n",
        "    gt_path       = '/content/tmp/TestDataset1/TestDataset/'\n",
        "    sal_path      = '/content/tmp/Predict_maps/'\n",
        "    test_datasets = ['NJU2K','NLPR', 'DES'] \n",
        "\n",
        "    def test_argument():\n",
        "      parser.add_argument('--methods',  type=str,  default=['SPNet'])\n",
        "      parser.add_argument('--datasets', type=str,  default=test_datasets)\n",
        "      parser.add_argument('--gt_dir',   type=str,  default=gt_path)\n",
        "      parser.add_argument('--root_dir', type=str,  default=sal_path)\n",
        "      parser.add_argument('--save_dir', type=str,  default=None)\n",
        "      parser.add_argument('--cuda',     type=bool, default=True)\n",
        "      return parser.parse_args(\"\")\n",
        "    \n",
        "    cfg = test_argument()\n",
        "    main(cfg)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "6Sd1YUBoS277",
        "outputId": "7653cf40-581c-4f28-ae14-1aa656c1f664"
      },
      "id": "6Sd1YUBoS277",
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-acb683aeadcf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0mcfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_argument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-43-acb683aeadcf>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEvalDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mosp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mosp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgt_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'GT'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m             \u001b[0mthread\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEval_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mthreads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthread\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-d907fea4e1b7>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, img_root, label_root)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_root\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_root\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_root\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/tmp/TestDataset1/TestDataset/SPNet/NJU2K'"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "Copy of SPNet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}